{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_data=pd.read_csv(\"Airbnb_Texas_Rentals.csv\",usecols=['average_rate_per_night', 'bedrooms_count', 'city',\n",
    "       'date_of_listing', 'description', 'latitude', 'longitude', 'title','url'],parse_dates=['date_of_listing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['average_rate_per_night', 'bedrooms_count', 'city', 'date_of_listing',\n",
       "       'description', 'latitude', 'longitude', 'title', 'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rate_per_night</th>\n",
       "      <th>bedrooms_count</th>\n",
       "      <th>city</th>\n",
       "      <th>date_of_listing</th>\n",
       "      <th>description</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$27</td>\n",
       "      <td>2</td>\n",
       "      <td>Humble</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>Welcome to stay in private room with queen bed...</td>\n",
       "      <td>30.020138</td>\n",
       "      <td>-95.293996</td>\n",
       "      <td>2 Private rooms/bathroom 10min from IAH airport</td>\n",
       "      <td>https://www.airbnb.com/rooms/18520444?location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  average_rate_per_night bedrooms_count    city date_of_listing  \\\n",
       "0                    $27              2  Humble      2016-05-01   \n",
       "\n",
       "                                         description   latitude  longitude  \\\n",
       "0  Welcome to stay in private room with queen bed...  30.020138 -95.293996   \n",
       "\n",
       "                                             title  \\\n",
       "0  2 Private rooms/bathroom 10min from IAH airport   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.airbnb.com/rooms/18520444?location...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18259, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_rate_per_night    28\n",
       "bedrooms_count             3\n",
       "city                       0\n",
       "date_of_listing            0\n",
       "description                2\n",
       "latitude                  34\n",
       "longitude                 34\n",
       "title                      3\n",
       "url                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null values of the dataset\n",
    "airbnb_data.isnull().sum()\n",
    "#average_rate_per_night -> replace NAN with 0, convert to int\n",
    "#bedrooms_count -> There are only 3 records so we decided to replace NAN with a category based on the desciption if it's possible. \n",
    "#description, latitude, longitude, title -> replace NAN to 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_rate_per_night            object\n",
       "bedrooms_count                    object\n",
       "city                              object\n",
       "date_of_listing           datetime64[ns]\n",
       "description                       object\n",
       "latitude                         float64\n",
       "longitude                        float64\n",
       "title                             object\n",
       "url                               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(airbnb_data):\n",
    "    \"\"\"\n",
    "    Method that removes nan values and imputes them\n",
    "    \n",
    "    Input: dataframe\n",
    "    Output: cleaned dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    #replace NAN with 0\n",
    "    airbnb_data.average_rate_per_night.replace(np.nan, '$0',inplace=True)\n",
    "    #convert to int and remove $\n",
    "    airbnb_data.average_rate_per_night=airbnb_data.average_rate_per_night.replace('[\\$]', '', regex=True).astype(int)\n",
    "\n",
    "    #replace NAN with'unknown'\n",
    "\n",
    "    airbnb_data.description.replace(np.nan,'unknown',inplace=True)\n",
    "    airbnb_data.title.replace(np.nan,'unknown',inplace=True)\n",
    "\n",
    "    airbnb_data.latitude.replace(np.nan,'unknown',inplace=True)\n",
    "    airbnb_data.longitude.replace(np.nan,'unknown',inplace=True)\n",
    "\n",
    "    #check where bedrooms_count doesn't have a value and save indexes of those records to a list\n",
    "    null_value_idx=airbnb_data[airbnb_data.bedrooms_count.isnull()].index\n",
    "    #if the word studio is mentioned in description then it is a studio otherwise 'unknown'\n",
    "    for idx in null_value_idx:\n",
    "        if 'studio' in airbnb_data.iloc[idx].description.split():\n",
    "            airbnb_data.bedrooms_count[idx]='Studio'\n",
    "        else:\n",
    "            airbnb_data.bedrooms_count[idx]='unknown'\n",
    "        \n",
    "    return airbnb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "average_rate_per_night    0\n",
       "bedrooms_count            0\n",
       "city                      0\n",
       "date_of_listing           0\n",
       "description               0\n",
       "latitude                  0\n",
       "longitude                 0\n",
       "title                     0\n",
       "url                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data=clean(airbnb_data)\n",
    "airbnb_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18259, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tsv_documents(airbnb_data):\n",
    "    \"\"\"\n",
    "    Method that creates different .tsv files for each record in the airbnb_data \n",
    "    \n",
    "    Input: dataframe\n",
    "    \"\"\"   \n",
    "    #clean data\n",
    "    airbnb_data=clean(airbnb_data)\n",
    "    \n",
    "    #for each index make a dataframe of airbnb_data and store it into new tsv file\n",
    "    for i in airbnb_data.index:\n",
    "        pd.DataFrame(airbnb_data.loc[i]).transpose().to_csv('data/doc_'+str(i)+'.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#method is run only once at the beginning to make separate .tsv files\n",
    "create_tsv_documents(airbnb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "1) Removing stop words\n",
    "\n",
    "2) Removing punctuation\n",
    "\n",
    "3) Stemming\n",
    "\n",
    "##### remove non english words and words Giulia chooses (room, price, airbnb) MOST often ones_?\n",
    "##### should we remove numbers__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1) Conjunctive query\n",
    "\n",
    "## 3.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(df):\n",
    "    #remove upper cases\n",
    "    df=df.lower()\n",
    "    #replacing new line sign '\\n' with a whitespace ' '    \n",
    "    df=df.replace('\\\\n',' ')\n",
    "\n",
    "    #removing stop words and punctuation\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    #for removing punctuations\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    #to tokenize the string\n",
    "    word_tokens = tokenizer.tokenize(df) \n",
    "\n",
    "    #stemming\n",
    "    ps = PorterStemmer()\n",
    "    filtered_words = [ps.stem(w) for w in word_tokens if not w in stop_words] \n",
    "\n",
    "    #remove non-english words\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a vocabulary\n",
    "\n",
    "#set for vocabulary (values of the set will be the keys fo vocabulary_dict)\n",
    "vocabulary_lst=[]\n",
    "#building a dictionary which will be used for making an inverted index\n",
    "doc_vocabs=defaultdict(list)\n",
    "\n",
    "for i in range(sample):\n",
    "    #take one file\n",
    "    df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "    #preprocessing \n",
    "    df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "    filtered_words=preprocessing_text(df)\n",
    "    temp_vocabulary_set=set()\n",
    "    for word in filtered_words:\n",
    "        temp_vocabulary_set.add(word)\n",
    "    vocabulary_lst.append(temp_vocabulary_set)\n",
    "    doc_vocabs[i]=list(temp_vocabulary_set)\n",
    "vocabulary_set=set.union(*vocabulary_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary={}\n",
    "for k,v in enumerate(vocabulary_set):\n",
    "    #just for testing\n",
    "    #vocabulary[v]='id'+str(k)\n",
    "    vocabulary[v]= k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(vocabulary): \n",
    "    \"\"\"\n",
    "    method that converts vocabulary into a dataframe and saves it into a csv file\n",
    "    \n",
    "    input: vocabulary(dictionary, key='term',value='term_id')\n",
    "    \"\"\"\n",
    "    vocabulary_dataframe=pd.DataFrame()\n",
    "    vocabulary_dataframe['word']=vocabulary.keys()\n",
    "    vocabulary_dataframe.to_csv('vocabulary_sample.csv')\n",
    "    del vocabulary_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocabulary(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute an inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inverted_idx(doc_vocabs,vocabulary):\n",
    "    \"\"\"\n",
    "    method that computes an inverted index\n",
    "    \n",
    "    input: doc_vocabs(dictionary), vocabulary(dictionary of all unique words, key=term, value=term_id)\n",
    "    output: inverted_idx(dictionary, key=term_id, value=list of document_ids) \n",
    "    \"\"\"\n",
    "    #initialize defaultdict for making an inverted index\n",
    "    inverted_idx = defaultdict(list)\n",
    "    #in every document look for every word and assign document id to the words which belong to it\n",
    "    for idx in doc_vocabs.keys():\n",
    "        for word in doc_vocabs[idx]:\n",
    "            inverted_idx[vocabulary[word]].append(idx)\n",
    "    return inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx=compute_inverted_idx(doc_vocabs,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a dictionary into a pickle file.\n",
    "import pickle\n",
    "\n",
    "pickle.dump(inverted_idx, open(\"inv_idx_sample.p\", \"wb\"))  # save it into a file named save.p\n",
    "\n",
    "# Load the dictionary back from the pickle file.\n",
    "\n",
    "inverted_index = pickle.load(open(\"inv_idx_sample.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_output(result_set):\n",
    "    df=pd.DataFrame()\n",
    "    for i,val in enumerate(result_set):\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "        df=df.append(pd.read_csv('data/doc_'+str(val)+'.tsv',sep='\\t',usecols=['description','title','city','url']\n",
    "                                 ,encoding='ISO-8859-1',index_col=False))\n",
    "        df.reset_index().drop('index',axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine():\n",
    "    user_query=str(input())\n",
    "    \n",
    "    user_query=preprocessing_text(user_query)\n",
    "\n",
    "    list_term_idx=[]\n",
    "    result_set=[]\n",
    "    for word in user_query:\n",
    "        #if word exist in the vocabulary\n",
    "        if word in vocabulary.keys():\n",
    "            list_term_idx.append(set(inverted_idx[vocabulary[word]]))\n",
    "        else:\n",
    "            list_term_idx.append({'x'})\n",
    "            break\n",
    "    result_set=list(set.intersection(*list_term_idx))\n",
    "    if 'x' in result_set or not result_set:\n",
    "        result_set='No results! Try again!'\n",
    "        return result_set\n",
    "        \n",
    "    print(result_set)\n",
    "    result_set=finalize_output(result_set)\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2) Conjunctive query & Ranking score\n",
    "### 3.2.1) Inverted index\n",
    "### 3.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate tfldf\n",
    "\n",
    "1) tf=term frequency -- the frequency of the word in each document in the corpus.\n",
    "\n",
    "2) idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc tfidf and cosine similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "Total_number_of_documents=airbnb_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "tf_dic=dict()\n",
    "for i in range(sample):\n",
    "    #take one file\n",
    "    df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "    #preprocessing \n",
    "    df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "    filtered_words=preprocessing_text(df)\n",
    "    tf_series=pd.Series(filtered_words)\n",
    "    tf_series=(tf_series.value_counts())/len(tf_series)\n",
    "    for index,value in tf_series.iteritems():\n",
    "        tf_dic[index,i]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "idf_dic=dict()\n",
    "total_num_docs_sample=sample\n",
    "for i in range(sample):\n",
    "    #take one file\n",
    "    df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "    #preprocessing \n",
    "    df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "    \n",
    "    filtered_words=preprocessing_text(df)\n",
    " #   for word_k in filtered_words_2: \n",
    "    idf_series=pd.Series(list(set(filtered_words)))\n",
    "    idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])) )\n",
    "    for idx in range(len(idf_series)):\n",
    "        idf_dic[idf_series[idx],i]=idf_calc[idx]      \n",
    "        \n",
    "  #      lis.append(np.log(total_num_docs_sample/len(inverted_idx[vocabulary[word_k]])))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "#preprocessing \n",
    "df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "filtered_words=preprocessing_text(df)\n",
    "\n",
    "tf_series=pd.Series(filtered_words)\n",
    "tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "#idf_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf_calc.sort_index()\n",
    "result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),idf_calc],axis=1)#.reset_index()\n",
    "result_df['res']=result_df[1]*result_df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antonio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.916291</td>\n",
       "      <td>0.114536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cozi</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entranc</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>histor</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>privat</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.203973</td>\n",
       "      <td>0.300993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>san</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>studio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1         2       res\n",
       "0  antonio  0.125  0.916291  0.114536\n",
       "1     cozi  0.125  1.609438  0.201180\n",
       "2  entranc  0.125  1.609438  0.201180\n",
       "3   histor  0.125  2.302585  0.287823\n",
       "4   privat  0.250  1.203973  0.300993\n",
       "5      san  0.125  2.302585  0.287823\n",
       "6   studio  0.125  2.302585  0.287823"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antonio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.916291</td>\n",
       "      <td>0.114536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cozi</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entranc</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>histor</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>privat</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.203973</td>\n",
       "      <td>0.300993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>san</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>studio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1         2       res\n",
       "0  antonio  0.125  0.916291  0.114536\n",
       "1     cozi  0.125  1.609438  0.201180\n",
       "2  entranc  0.125  1.609438  0.201180\n",
       "3   histor  0.125  2.302585  0.287823\n",
       "4   privat  0.250  1.203973  0.300993\n",
       "5      san  0.125  2.302585  0.287823\n",
       "6   studio  0.125  2.302585  0.287823"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "tf_idf_dic=dict()\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "idf_dic2={}\n",
    "tf_dic2={}\n",
    "proba={}\n",
    "#OVDE\n",
    "total_num_docs_sample=sample\n",
    "result_df=pd.DataFrame()\n",
    "for i in range(sample):\n",
    "    #take one file\n",
    "    df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "    #preprocessing \n",
    "    df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "    filtered_words=preprocessing_text(df)\n",
    "    tf_series=pd.Series(filtered_words)\n",
    "    tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "    idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "    idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "    #idf_calc.sort_index()\n",
    "    result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),pd.Series(idf_calc.values)],axis=1)#.reset_index()\n",
    "#    result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),idf_calc],axis=1)#.reset_index()\n",
    "    result_df['tf_idf']=result_df[1]*result_df[2]\n",
    "   # result_df=result_df.loc[:,['res',0]]\n",
    "    \n",
    "    for idx in range(result_df.shape[0]):\n",
    "        tf_idf_dic[result_df[0][idx],i]=result_df['tf_idf'][idx]\n",
    "#del result_df\n",
    "    for idx in range(len(tf_series)):\n",
    "#        tf_idf_dic[tf_series.index[idx],i]=tf_series[idx]*idf_calc[idx]\n",
    "        idf_dic2[idf_series[idx],i]=idf_calc[idx] \n",
    "    for index,value in tf_series.iteritems():\n",
    "        tf_dic2[index,i]=value\n",
    "    for k in tf_dic2.keys():\n",
    "        proba[k]=tf_dic2[k]*idf_dic2[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antonio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cozi</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entranc</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>histor</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>privat</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.916291</td>\n",
       "      <td>0.229073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>san</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.203973</td>\n",
       "      <td>0.150497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>studio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1         2    tf_idf\n",
       "0  antonio  0.125  1.609438  0.201180\n",
       "1     cozi  0.125  1.609438  0.201180\n",
       "2  entranc  0.125  2.302585  0.287823\n",
       "3   histor  0.125  2.302585  0.287823\n",
       "4   privat  0.250  0.916291  0.229073\n",
       "5      san  0.125  1.203973  0.150497\n",
       "6   studio  0.125  2.302585  0.287823"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.60943791, 1.60943791, 2.30258509, 2.30258509, 0.91629073,\n",
       "       1.2039728 , 2.30258509])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_calc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tf_dic.keys():\n",
    "    if(tf_dic[key]*idf_dic[key]!=proba[key]):\n",
    " #       print(key,tf_dic[key])\n",
    "#        print(key,idf_dic[key])\n",
    "        print(key,tf_dic[key]*idf_dic[key])\n",
    "        print(key,tf_idf_dic[key])\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05733203830123505"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba['san',2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tf_dic.keys():\n",
    "    if(tf_dic[key]*idf_dic[key]!=tf_idf_dic[key]):\n",
    " #       print(key,tf_dic[key])\n",
    "#        print(key,idf_dic[key])\n",
    "        print(key,tf_dic[key]*idf_dic[key])\n",
    "        print(key,tf_idf_dic[key])\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First way\n",
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "def calculate_tf_idf(airbnb_data):\n",
    "    tf_idf_dic=dict()\n",
    "    total_num_docs_sample=sample\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "        result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),pd.Series(idf_calc.values)],axis=1)#.reset_index()\n",
    "        result_df['tf_idf']=result_df[1]*result_df[2]\n",
    "\n",
    "        for idx in range(result_df.shape[0]):\n",
    "            tf_idf_dic[result_df[0][idx],i]=result_df['tf_idf'][idx]\n",
    "    return tf_idf_dic        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second way--to check if it is the same like the 1st-for double checking the results\n",
    "def calculate_tf_idf2(airbnb_data):\n",
    "    idf_dic2={}\n",
    "    tf_dic2={}\n",
    "    proba={}\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "       \n",
    "        for idx in range(len(tf_series)):\n",
    "            idf_dic2[idf_series[idx],i]=idf_calc[idx] \n",
    "        for index,value in tf_series.iteritems():\n",
    "            tf_dic2[index,i]=value\n",
    "        for k in tf_dic2.keys():\n",
    "            proba[k]=tf_dic2[k]*idf_dic2[k]\n",
    "    return proba        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=calculate_tf_idf(airbnb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=calculate_tf_idf2(airbnb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in a.keys():\n",
    "    if(a[k]!=b[k]):\n",
    "        print('њет')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inverted_idx(doc_vocabs,vocabulary):\n",
    "    \"\"\"\n",
    "    method that computes an inverted index\n",
    "    \n",
    "    input: doc_vocabs(dictionary), vocabulary(dictionary of all unique words, key=term, value=term_id)\n",
    "    output: inverted_idx(dictionary, key=term_id, value=list of document_ids) \n",
    "    \"\"\"\n",
    "    #initialize defaultdict for making an inverted index\n",
    "    inverted_idx = defaultdict(list)\n",
    "    #in every document look for every word and assign document id to the words which belong to it\n",
    "    for idx in doc_vocabs.keys():\n",
    "        for word in doc_vocabs[idx]:\n",
    "            inverted_idx[vocabulary[word]].append(idx)\n",
    "    return inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx=compute_inverted_idx(doc_vocabs,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=('10', 0)\n",
    "aa=aa[1]\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70710678],\n",
       "       [0.70710678, 1.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=[1,2]\n",
    "Y=[3,1]\n",
    "cosine_similarity([X,Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2=defaultdict(list)\n",
    "for k,v in a.items():\n",
    "    k=k[1]\n",
    "    a2[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [0.11808128682020748,\n",
       "              0.11808128682020748,\n",
       "              0.04126763878036154,\n",
       "              0.11808128682020748,\n",
       "              0.09261329264045663,\n",
       "              0.05904064341010374,\n",
       "              0.06174219509363775,\n",
       "              0.04698926830123872,\n",
       "              0.08253527756072308,\n",
       "              0.017773004629742187,\n",
       "              0.05904064341010374,\n",
       "              0.05904064341010374,\n",
       "              0.04126763878036154,\n",
       "              0.11808128682020748,\n",
       "              0.05904064341010374,\n",
       "              0.11808128682020748,\n",
       "              0.05904064341010374,\n",
       "              0.09397853660247744,\n",
       "              0.04126763878036154,\n",
       "              0.035546009259484375,\n",
       "              0.05904064341010374,\n",
       "              0.05904064341010374,\n",
       "              0.05904064341010374,\n",
       "              0.05904064341010374,\n",
       "              0.04126763878036154],\n",
       "             1: [0.025147467381782817,\n",
       "              0.03597789207803197,\n",
       "              0.07195578415606393,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.018812075067592752,\n",
       "              0.07195578415606393,\n",
       "              0.03597789207803197,\n",
       "              0.025147467381782817,\n",
       "              0.07195578415606393,\n",
       "              0.014317042685533674,\n",
       "              0.025147467381782817,\n",
       "              0.010830424696249145,\n",
       "              0.03597789207803197,\n",
       "              0.014317042685533674,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.050294934763565634,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.025147467381782817,\n",
       "              0.03597789207803197,\n",
       "              0.07195578415606393,\n",
       "              0.010830424696249145,\n",
       "              0.018812075067592752,\n",
       "              0.03597789207803197,\n",
       "              0.07195578415606393,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.1079336762340959,\n",
       "              0.03597789207803197,\n",
       "              0.014317042685533674,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.025147467381782817,\n",
       "              0.018812075067592752,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.025147467381782817,\n",
       "              0.03597789207803197,\n",
       "              0.010830424696249145,\n",
       "              0.018812075067592752,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.025147467381782817,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197],\n",
       "             2: [0.2192938183803853,\n",
       "              0.04363289199400738,\n",
       "              0.10964690919019265,\n",
       "              0.17199611490370514,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.4385876367607706,\n",
       "              0.03300700859809263,\n",
       "              0.05733203830123505,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265],\n",
       "             3: [0.10964690919019265,\n",
       "              0.0766399005921,\n",
       "              0.04363289199400738,\n",
       "              0.06601401719618526,\n",
       "              0.2192938183803853,\n",
       "              0.10964690919019265,\n",
       "              0.04363289199400738,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.0766399005921,\n",
       "              0.03300700859809263,\n",
       "              0.10964690919019265,\n",
       "              0.04363289199400738,\n",
       "              0.17453156797602953,\n",
       "              0.03300700859809263,\n",
       "              0.10964690919019265],\n",
       "             4: [0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.09754169166267275,\n",
       "              0.06977530584830442,\n",
       "              0.09754169166267275,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06301338005090412,\n",
       "              0.03648402437351322,\n",
       "              0.06977530584830442,\n",
       "              0.048770845831336375,\n",
       "              0.09754169166267275,\n",
       "              0.06977530584830442,\n",
       "              0.03648402437351322,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.048770845831336375,\n",
       "              0.048770845831336375,\n",
       "              0.09754169166267275],\n",
       "             5: [0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.031596232133591556,\n",
       "              0.15879897193062387,\n",
       "              0.05549785904945173,\n",
       "              0.07939948596531193,\n",
       "              0.15879897193062387,\n",
       "              0.05549785904945173,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.02390162691586018,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.031596232133591556,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.23819845789593577,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193],\n",
       "             6: [0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.04598394035526001,\n",
       "              0.06578814551411559,\n",
       "              0.03439922298074103,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.13157629102823118,\n",
       "              0.03439922298074103,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.09196788071052002,\n",
       "              0.04598394035526001,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.01980420515885558,\n",
       "              0.06578814551411559,\n",
       "              0.03439922298074103,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.04598394035526001,\n",
       "              0.06578814551411559,\n",
       "              0.03439922298074103,\n",
       "              0.06578814551411559,\n",
       "              0.13157629102823118,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559],\n",
       "             7: [0.08223518189264449,\n",
       "              0.042999028725926286,\n",
       "              0.03272466899550554,\n",
       "              0.05747992544407501,\n",
       "              0.05747992544407501,\n",
       "              0.024755256448569473,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.05747992544407501,\n",
       "              0.05747992544407501,\n",
       "              0.05747992544407501,\n",
       "              0.042999028725926286,\n",
       "              0.042999028725926286,\n",
       "              0.03272466899550554,\n",
       "              0.08223518189264449,\n",
       "              0.03272466899550554,\n",
       "              0.05747992544407501,\n",
       "              0.05747992544407501,\n",
       "              0.024755256448569473,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.05747992544407501],\n",
       "             8: [0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.03465735902799726,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.2302585092994046,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.2302585092994046,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023],\n",
       "             9: [0.20117973905426254,\n",
       "              0.20117973905426254,\n",
       "              0.28782313662425574,\n",
       "              0.28782313662425574,\n",
       "              0.22907268296853878,\n",
       "              0.15049660054074201,\n",
       "              0.28782313662425574]})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(0.92,0.3200323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spatial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-faa2dad44dc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#TFIDF_query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.1446624\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m0.72522043\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.74339473\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spatial' is not defined"
     ]
    }
   ],
   "source": [
    "a=[0.05215743696136418,\n",
    "  0.03257057712576686,\n",
    "  0.032268990804419175]\n",
    "#TFIDF_query\n",
    "b=np.array([1.1446624 , 0.72522043, 0.74339473])\n",
    "1 - spatial.distance.cosine(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "dataSetI = [0.03597789207803197\n",
    "           # ,0.03597789207803197,\n",
    "            #  0.03597789207803197,\n",
    "            #  0.03597789207803197\n",
    "           ]\n",
    "dataSetII = [0.03597789207803197#,\n",
    "             # 0.03597789207803197,\n",
    "             # 1.03597789207803197,\n",
    "             # 2.03597789207803197\n",
    "            ]\n",
    "result = 1 - spatial.distance.cosine(dataSetI, dataSetII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2=defaultdict(tuple)\n",
    "for term_id in inverted_idx.keys():\n",
    "    for k,v in vocabulary.items():\n",
    "        if v==term_id:\n",
    "            term=k\n",
    "    for doc_id in range(len(inverted_idx[term_id])):\n",
    "        inverted_idx[term_id].append(\n",
    "            (doc_id,\n",
    "             a[term,doc_id]))\n",
    "        inverted_idx[term_id].remove(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id in inverted_idx[term_id]:\n",
    "    print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a dictionary into a pickle file.\n",
    "import pickle\n",
    "\n",
    "pickle.dump(inverted_idx, open(\"inv_idx2_sample.p\", \"wb\"))  # save it into a file named save.p\n",
    "\n",
    "# Load the dictionary back from the pickle file.\n",
    "\n",
    "inverted_index = pickle.load(open(\"inv_idx2_sample.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INVERTED INDEX 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic=calculate_tf_idf(airbnb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2=defaultdict(list)\n",
    "for term_id in inverted_idx.keys():\n",
    "    for k,v in vocabulary.items():#k->term, v->term_id\n",
    "        if v==term_id:\n",
    "            term=k\n",
    "    for doc_id in inverted_idx[term_id]:\n",
    "        inverted_idx2[term_id].append((doc_id,tf_idf_dic[term,doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary['airport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index[vocabulary['bathroom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverted_idx2[vocabulary['bathroom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query=\"room bedroom\"\n",
    "#str(input())\n",
    "user_query=preprocessing_text(user_query)\n",
    "user_query_tfidf=np.ones(len(user_query))\n",
    "\n",
    "\n",
    "list_term_idx=[]\n",
    "#list of dataframes\n",
    "list_tf_idf=[]\n",
    "\n",
    "result_set=[]\n",
    "\n",
    "result_tf_idf_dic=defaultdict(list)\n",
    "for word in user_query:\n",
    "    #if word exist in the vocabulary\n",
    "    if word in vocabulary.keys():\n",
    "        list_term_idx.append(set(inverted_idx[vocabulary[word]]))\n",
    "        list_tf_idf.append((inverted_idx2[vocabulary[word]]))#[:,1])\n",
    "        #result_tf_idf_dic\n",
    "    else:\n",
    "        list_term_idx.append({'x'})\n",
    "        break\n",
    "result_set=list(set.intersection(*list_term_idx))\n",
    "if 'x' in result_set or not result_set:\n",
    "    result_set='No results! Try again!'\n",
    "\n",
    "tf_idf_dic=defaultdict(list)\n",
    "\n",
    "for tf_idf_1doc in list_tf_idf:\n",
    "    for tuple_pair in tf_idf_1doc:\n",
    "        if tuple_pair[0] in result_set:\n",
    "            tf_idf_dic[tuple_pair[0]].append(tuple_pair[1])\n",
    "\n",
    "print(result_set)\n",
    "#result_set=finalize_output(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst=[]\n",
    "cosine_similarity_dic={}\n",
    "for value in tf_idf_dic.values():\n",
    "    print(cosine_similarity([user_query_tfidf],[value]))\n",
    "    cosine_similarity_lst.append(cosine_similarity([user_query_tfidf],[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop, nlargest,heapify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap = []\n",
    "for item in cosine_similarity_lst:\n",
    "     heappush(heap, item)\n",
    "#heapify(cosine_similarity_lst)\n",
    "#ordered = []\n",
    "#while heap:\n",
    "#    ordered.append(heappop(heap))\n",
    "#ordered\n",
    "#heap.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlargest(3,heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst.hea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-cosine_similarity([np.array(a)],[np.array(b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a, b):\n",
    "    cosine_similarity=(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "    return 1 - cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distance([1,1,1],[0.03554600925948437,0.5,0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2[14][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-15aa7aeb07c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muser_query\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rooma'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'user_query' is not defined"
     ]
    }
   ],
   "source": [
    "user_query.index('rooma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inverted_idx2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-3e48337b57fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minverted_idx2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'room'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'doc_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tf_idf'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inverted_idx2' is not defined"
     ]
    }
   ],
   "source": [
    "a=pd.DataFrame(inverted_idx2[vocabulary['room']],columns=['doc_id','tf_idf'+str(user_query.index(word))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inverted_idx2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-e76fafceb68b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minverted_idx2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bedroom'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'doc_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tf_idf'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inverted_idx2' is not defined"
     ]
    }
   ],
   "source": [
    "b=pd.DataFrame(inverted_idx2[vocabulary['bedroom']],columns=['doc_id','tf_idf'+str(user_query.index(word))])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can not merge DataFrame with instance of type <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-64861968631f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'doc_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'doc_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[0;32m     51\u001b[0m                          \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                          copy=copy, indicator=indicator)\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[0;32m    529\u001b[0m             raise ValueError(\n\u001b[0;32m    530\u001b[0m                 \u001b[1;34m'can not merge DataFrame with instance of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                 'type {0}'.format(type(left)))\n\u001b[0m\u001b[0;32m    532\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: can not merge DataFrame with instance of type <class 'list'>"
     ]
    }
   ],
   "source": [
    "pd.merge(*c,left_on='doc_id',right_on='doc_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inverted_idx2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-3c45a8758395>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minverted_idx2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m23\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'inverted_idx2' is not defined"
     ]
    }
   ],
   "source": [
    "np.array(inverted_idx2[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inverted_idx2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-1b98cafa23ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minverted_idx2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'inverted_idx2' is not defined"
     ]
    }
   ],
   "source": [
    "inverted_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_term_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-b5377f848d0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist_term_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'list_term_idx' is not defined"
     ]
    }
   ],
   "source": [
    "list_term_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_output(result_set):\n",
    "    df=pd.DataFrame()\n",
    "    for i,val in enumerate(result_set):\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "        df=df.append(pd.read_csv('data/doc_'+str(val)+'.tsv',sep='\\t',usecols=['description','title','city','url']\n",
    "                                 ,encoding='ISO-8859-1',index_col=False))\n",
    "        df.reset_index().drop('index',axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine():\n",
    "    user_query=str(input())\n",
    "    \n",
    "    user_query=preprocessing_text(user_query)\n",
    "\n",
    "    list_term_idx=[]\n",
    "    result_set=[]\n",
    "    for word in user_query:\n",
    "        #if word exist in the vocabulary\n",
    "        if word in vocabulary.keys():\n",
    "            list_term_idx.append(set(inverted_idx[vocabulary[word]]))\n",
    "        else:\n",
    "            list_term_idx.append({'x'})\n",
    "            break\n",
    "    result_set=list(set.intersection(*list_term_idx))\n",
    "    if 'x' in result_set or not result_set:\n",
    "        result_set='No results! Try again!'\n",
    "        return result_set\n",
    "        \n",
    "    print(result_set)\n",
    "    result_set=finalize_output(result_set)\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "descriptor 'intersection' of 'set' object needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-3c09e78999cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msearch_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-1b149dd6500a>\u001b[0m in \u001b[0;36msearch_engine\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mlist_term_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mresult_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlist_term_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'x'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult_set\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mresult_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'No results! Try again!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: descriptor 'intersection' of 'set' object needs an argument"
     ]
    }
   ],
   "source": [
    "search_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more concise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2) Conjunctive query & Ranking score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1) Inverted index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First way\n",
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "def calculate_tf_idf(airbnb_data):\n",
    "    tf_idf_dic=dict()\n",
    "    total_num_docs_sample=sample\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "        result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),pd.Series(idf_calc.values)],axis=1)#.reset_index()\n",
    "        result_df['tf_idf']=result_df[1]*result_df[2]\n",
    "\n",
    "        for idx in range(result_df.shape[0]):\n",
    "            tf_idf_dic[result_df[0][idx],i]=result_df['tf_idf'][idx]\n",
    "    return tf_idf_dic        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second way--to check if it is the same like the 1st-for double checking the results\n",
    "def calculate_tf_idf2(airbnb_data):\n",
    "    idf_dic2={}\n",
    "    tf_dic2={}\n",
    "    proba={}\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "       \n",
    "        for idx in range(len(tf_series)):\n",
    "            idf_dic2[idf_series[idx],i]=idf_calc[idx] \n",
    "        for index,value in tf_series.iteritems():\n",
    "            tf_dic2[index,i]=value\n",
    "        for k in tf_dic2.keys():\n",
    "            proba[k]=tf_dic2[k]*idf_dic2[k]\n",
    "    return proba        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic=calculate_tf_idf(airbnb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2=defaultdict(list)\n",
    "for term_id in inverted_idx.keys():\n",
    "    for k,v in vocabulary.items():#k->term, v->term_id\n",
    "        if v==term_id:\n",
    "            term=k\n",
    "    for doc_id in inverted_idx[term_id]:\n",
    "        inverted_idx2[term_id].append((doc_id,tf_idf_dic[term,doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverted_idx2[vocabulary['bathroom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst=[]\n",
    "for value in tf_idf_dic.values():\n",
    "    tf_idf_val=cosine_similarity([user_query_tfidf],[value])[0][0]\n",
    "    #keys()\n",
    "    cosine_similarity_lst.append(tf_idf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop, nlargest,heapify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst_tuples=[]\n",
    "for key,value in tf_idf_dic.items():\n",
    "    tf_idf_val=cosine_similarity([user_query_tfidf],[value])[0][0]\n",
    "    cosine_similarity_lst_tuples.append((tf_idf_val,key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a, b):\n",
    "    cosine_similarity=(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "    return 1 - cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3391904192171833"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_distance([1,1,1],[0.03554600925948437,0.5,0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cosine_similarity_lst_tuples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-c3bd3d70278c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mheap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcosine_similarity_lst_tuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m      \u001b[0mheappush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#heap.sort(reverse=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnlargest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cosine_similarity_lst_tuples' is not defined"
     ]
    }
   ],
   "source": [
    "heap = []\n",
    "for item in cosine_similarity_lst_tuples:\n",
    "     heappush(heap, item)\n",
    "#heap.sort(reverse=True)\n",
    "nlargest(3,heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlargest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-0c0052472c0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nlargest' is not defined"
     ]
    }
   ],
   "source": [
    "(nlargest(3,heap))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "user_query=\"room bedroom\"\n",
    "#str(input())\n",
    "user_query=preprocessing_text(user_query)\n",
    "user_query_tfidf=np.ones(len(user_query))\n",
    "\n",
    "list_term_idx=[]\n",
    "#list of dataframes\n",
    "list_tf_idf=[]\n",
    "\n",
    "result_set=[]\n",
    "\n",
    "result_tf_idf_dic=defaultdict(list)\n",
    "for word in user_query:\n",
    "    #if word exist in the vocabulary\n",
    "    if word in vocabulary.keys():\n",
    "        list_term_idx.append(set(inverted_idx[vocabulary[word]]))\n",
    "        list_tf_idf.append((inverted_idx2[vocabulary[word]]))#[:,1])\n",
    "        #result_tf_idf_dic\n",
    "    else:\n",
    "        list_term_idx.append({'x'})\n",
    "        break\n",
    "result_set=list(set.intersection(*list_term_idx))\n",
    "if 'x' in result_set or not result_set:\n",
    "    result_set='No results! Try again!'\n",
    "\n",
    "tf_idf_dic=defaultdict(list)\n",
    "\n",
    "for tf_idf_1doc in list_tf_idf:\n",
    "    for tuple_pair in tf_idf_1doc:\n",
    "        if tuple_pair[0] in result_set:\n",
    "            tf_idf_dic[tuple_pair[0]].append(tuple_pair[1])\n",
    "\n",
    "print(result_set)\n",
    "#result_set=finalize_output(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_doc_ids={}\n",
    "for tup in (nlargest(3,heap)):\n",
    "    wanted_doc_ids[tup[1]]=tup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1.0, 3: 0.9486832980505138, 7: 1.0}"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wanted_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([7, 1, 3])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_set=wanted_doc_ids.keys()\n",
    "result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([7, 1, 3])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "\n",
    "for i,val in enumerate(result_set):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    df=df.append(pd.read_csv('data/doc_'+str(val)+'.tsv',sep='\\t',usecols=['description','title','city','url']\n",
    "                             ,encoding='ISO-8859-1',index_col=False))\n",
    "    df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['similarity']=wanted_doc_ids.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>city</th>\n",
       "      <th>url</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Friendly Private Room in ?Quiet Neighborhood</td>\n",
       "      <td>This is a beautiful bedroom with a queen size bed and closet. We do not have pets and the house is always clean. The bathroom is shared and supplies such as towels and shampoo are available. We are only some miles from Downtown, TCU, TCC, and Stockyards.</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>https://www.airbnb.com/rooms/18977363?location=Cleburne%2C%20TX</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unique Location! Alamo Heights - Designer Inspired</td>\n",
       "      <td>Stylish, fully remodeled home in upscale NW  Alamo Heights Area. \\n\\nAmazing location - House conveniently located in quiet street, with beautiful seasoned trees, prestigious neighborhood and very close to the airport, 281, 410 loop and down-town area. \\n\\nFeaturing an open floor plan, original hardwood floors, 3 bedrooms, 3 FULL bathrooms + an independent garden-TV room which can sleep 2 more\\n\\nEuropean inspired kitchen and top of the line decor. Driveway can park 4 cars.</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>https://www.airbnb.com/rooms/17481455?location=Cibolo%2C%20TX</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private Room Close to Campus</td>\n",
       "      <td>Private bedroom in a cute little home situated in the coveted Garden Acres neighborhood in Bryan. The bedroom has its own private access and its own private bathroom.</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>https://www.airbnb.com/rooms/11839729?location=College%20Station%2C%20TX</td>\n",
       "      <td>0.948683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0  Friendly Private Room in ?Quiet Neighborhood         \n",
       "0  Unique Location! Alamo Heights - Designer Inspired   \n",
       "0  Private Room Close to Campus                         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         description  \\\n",
       "0  This is a beautiful bedroom with a queen size bed and closet. We do not have pets and the house is always clean. The bathroom is shared and supplies such as towels and shampoo are available. We are only some miles from Downtown, TCU, TCC, and Stockyards.                                                                                                                                                                                                                                      \n",
       "0  Stylish, fully remodeled home in upscale NW  Alamo Heights Area. \\n\\nAmazing location - House conveniently located in quiet street, with beautiful seasoned trees, prestigious neighborhood and very close to the airport, 281, 410 loop and down-town area. \\n\\nFeaturing an open floor plan, original hardwood floors, 3 bedrooms, 3 FULL bathrooms + an independent garden-TV room which can sleep 2 more\\n\\nEuropean inspired kitchen and top of the line decor. Driveway can park 4 cars.   \n",
       "0  Private bedroom in a cute little home situated in the coveted Garden Acres neighborhood in Bryan. The bedroom has its own private access and its own private bathroom.                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "          city  \\\n",
       "0  Fort Worth    \n",
       "0  San Antonio   \n",
       "0  Bryan         \n",
       "\n",
       "                                                                        url  \\\n",
       "0  https://www.airbnb.com/rooms/18977363?location=Cleburne%2C%20TX            \n",
       "0  https://www.airbnb.com/rooms/17481455?location=Cibolo%2C%20TX              \n",
       "0  https://www.airbnb.com/rooms/11839729?location=College%20Station%2C%20TX   \n",
       "\n",
       "   similarity  \n",
       "0  1.000000    \n",
       "0  1.000000    \n",
       "0  0.948683    "
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title','description','city','url','similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sort_values('Similarity',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2) Conjunctive query & Ranking score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1) Inverted index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First way\n",
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "def calculate_tf_idf(airbnb_data):\n",
    "    tf_idf_dic=dict()\n",
    "    total_num_docs_sample=sample\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "        result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),pd.Series(idf_calc.values)],axis=1)#.reset_index()\n",
    "        result_df['tf_idf']=result_df[1]*result_df[2]\n",
    "\n",
    "        for idx in range(result_df.shape[0]):\n",
    "            tf_idf_dic[result_df[0][idx],i]=result_df['tf_idf'][idx]\n",
    "    return tf_idf_dic        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second way--to check if it is the same like the 1st-for double checking the results\n",
    "def calculate_tf_idf2(airbnb_data):\n",
    "    idf_dic2={}\n",
    "    tf_dic2={}\n",
    "    proba={}\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "       \n",
    "        for idx in range(len(tf_series)):\n",
    "            idf_dic2[idf_series[idx],i]=idf_calc[idx] \n",
    "        for index,value in tf_series.iteritems():\n",
    "            tf_dic2[index,i]=value\n",
    "        for k in tf_dic2.keys():\n",
    "            proba[k]=tf_dic2[k]*idf_dic2[k]\n",
    "    return proba        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic=calculate_tf_idf(airbnb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inverted_idx2(inverted_idx,vocabulary,tf_idf_dic):\n",
    "    \"\"\"\n",
    "    method that computes the second inverted index\n",
    "    \n",
    "    input:  \n",
    "    output: inverted_idx2(dictionary, key=term_id, value=list of tuples (document_id,tf_idf value) )\n",
    "    \"\"\"\n",
    "    inverted_idx2=defaultdict(list)\n",
    "    for term_id in inverted_idx.keys():\n",
    "        for k,v in vocabulary.items():#k->term, v->term_id\n",
    "            if v==term_id:\n",
    "                term=k\n",
    "        for doc_id in inverted_idx[term_id]:\n",
    "            inverted_idx2[term_id].append((doc_id,tf_idf_dic[term,doc_id]))\n",
    "    return inverted_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2=compute_inverted_idx2(inverted_idx,vocabulary,tf_idf_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [(1, 0.025147467381782817), (4, 0.09754169166267275)],\n",
       "             1: [(3, 0.10964690919019265)],\n",
       "             2: [(6, 0.06578814551411559)],\n",
       "             3: [(0, 0.11808128682020748)],\n",
       "             4: [(1, 0.03597789207803197)],\n",
       "             5: [(1, 0.018812075067592752),\n",
       "              (4, 0.03648402437351322),\n",
       "              (6, 0.03439922298074103)],\n",
       "             6: [(0, 0.05904064341010374)],\n",
       "             7: [(3, 0.10964690919019265)],\n",
       "             8: [(6, 0.06578814551411559)],\n",
       "             9: [(6, 0.06578814551411559)],\n",
       "             10: [(5, 0.07939948596531193)],\n",
       "             11: [(0, 0.06174219509363775),\n",
       "              (6, 0.03439922298074103),\n",
       "              (7, 0.042999028725926286)],\n",
       "             12: [(4, 0.06977530584830442)],\n",
       "             13: [(6, 0.06578814551411559)],\n",
       "             14: [(1, 0.025147467381782817), (9, 0.20117973905426254)],\n",
       "             15: [(0, 0.017773004629742187),\n",
       "              (1, 0.010830424696249145),\n",
       "              (3, 0.06601401719618526),\n",
       "              (7, 0.024755256448569473),\n",
       "              (8, 0.03465735902799726)],\n",
       "             16: [(0, 0.05904064341010374)],\n",
       "             17: [(0, 0.11808128682020748)],\n",
       "             18: [(6, 0.06578814551411559)],\n",
       "             19: [(2, 0.10964690919019265)],\n",
       "             20: [(3, 0.10964690919019265)],\n",
       "             21: [(4, 0.06977530584830442)],\n",
       "             22: [(1, 0.03597789207803197)],\n",
       "             23: [(1, 0.07195578415606393)],\n",
       "             24: [(0, 0.05904064341010374)],\n",
       "             25: [(1, 0.03597789207803197)],\n",
       "             26: [(4, 0.06977530584830442)],\n",
       "             27: [(2, 0.10964690919019265)],\n",
       "             28: [(4, 0.06977530584830442)],\n",
       "             29: [(6, 0.06578814551411559)],\n",
       "             30: [(7, 0.08223518189264449)],\n",
       "             31: [(3, 0.0766399005921), (6, 0.04598394035526001)],\n",
       "             32: [(8, 0.1151292546497023)],\n",
       "             33: [(1, 0.07195578415606393)],\n",
       "             34: [(6, 0.06578814551411559)],\n",
       "             35: [(1, 0.03597789207803197)],\n",
       "             36: [(6, 0.06578814551411559)],\n",
       "             37: [(6, 0.06578814551411559)],\n",
       "             38: [(7, 0.08223518189264449)],\n",
       "             39: [(9, 0.28782313662425574)],\n",
       "             40: [(5, 0.07939948596531193)],\n",
       "             41: [(1, 0.03597789207803197)],\n",
       "             42: [(1, 0.07195578415606393)],\n",
       "             43: [(5, 0.07939948596531193)],\n",
       "             44: [(5, 0.07939948596531193)],\n",
       "             45: [(8, 0.1151292546497023)],\n",
       "             46: [(1, 0.03597789207803197)],\n",
       "             47: [(2, 0.10964690919019265)],\n",
       "             48: [(5, 0.05549785904945173), (6, 0.09196788071052002)],\n",
       "             49: [(0, 0.05904064341010374)],\n",
       "             50: [(0, 0.05904064341010374)],\n",
       "             51: [(1, 0.03597789207803197)],\n",
       "             52: [(8, 0.1151292546497023)],\n",
       "             53: [(0, 0.09397853660247744),\n",
       "              (3, 0.17453156797602953),\n",
       "              (7, 0.03272466899550554),\n",
       "              (9, 0.22907268296853878)],\n",
       "             54: [(4, 0.06977530584830442)],\n",
       "             55: [(4, 0.06977530584830442)],\n",
       "             56: [(5, 0.23819845789593577)],\n",
       "             57: [(8, 0.1151292546497023)],\n",
       "             58: [(0, 0.04126763878036154), (1, 0.050294934763565634)],\n",
       "             59: [(0, 0.05904064341010374)],\n",
       "             60: [(4, 0.048770845831336375), (6, 0.04598394035526001)],\n",
       "             61: [(6, 0.06578814551411559)],\n",
       "             62: [(7, 0.08223518189264449)],\n",
       "             63: [(0, 0.11808128682020748)],\n",
       "             64: [(6, 0.06578814551411559)],\n",
       "             65: [(6, 0.13157629102823118)],\n",
       "             66: [(4, 0.09754169166267275), (7, 0.05747992544407501)],\n",
       "             67: [(1, 0.03597789207803197)],\n",
       "             68: [(0, 0.11808128682020748)],\n",
       "             69: [(1, 0.03597789207803197)],\n",
       "             70: [(6, 0.06578814551411559)],\n",
       "             71: [(4, 0.09754169166267275), (7, 0.05747992544407501)],\n",
       "             72: [(4, 0.06977530584830442)],\n",
       "             73: [(6, 0.06578814551411559)],\n",
       "             74: [(1, 0.025147467381782817), (7, 0.05747992544407501)],\n",
       "             75: [(7, 0.08223518189264449)],\n",
       "             76: [(4, 0.06977530584830442)],\n",
       "             77: [(0, 0.04126763878036154), (7, 0.05747992544407501)],\n",
       "             78: [(1, 0.010830424696249145),\n",
       "              (3, 0.03300700859809263),\n",
       "              (4, 0.06301338005090412),\n",
       "              (5, 0.02390162691586018),\n",
       "              (6, 0.01980420515885558)],\n",
       "             79: [(1, 0.03597789207803197)],\n",
       "             80: [(5, 0.15879897193062387)],\n",
       "             81: [(7, 0.08223518189264449)],\n",
       "             82: [(0, 0.05904064341010374)],\n",
       "             83: [(8, 0.1151292546497023)],\n",
       "             84: [(5, 0.07939948596531193)],\n",
       "             85: [(2, 0.4385876367607706)],\n",
       "             86: [(0, 0.08253527756072308), (7, 0.05747992544407501)],\n",
       "             87: [(1, 0.018812075067592752),\n",
       "              (2, 0.05733203830123505),\n",
       "              (9, 0.15049660054074201)],\n",
       "             88: [(6, 0.06578814551411559)],\n",
       "             89: [(6, 0.06578814551411559)],\n",
       "             90: [(1, 0.03597789207803197)],\n",
       "             91: [(4, 0.09754169166267275), (7, 0.05747992544407501)],\n",
       "             92: [(1, 0.03597789207803197)],\n",
       "             93: [(5, 0.07939948596531193)],\n",
       "             94: [(8, 0.1151292546497023)],\n",
       "             95: [(4, 0.06977530584830442)],\n",
       "             96: [(5, 0.07939948596531193)],\n",
       "             97: [(1, 0.03597789207803197)],\n",
       "             98: [(3, 0.10964690919019265)],\n",
       "             99: [(2, 0.10964690919019265)],\n",
       "             100: [(0, 0.04698926830123872),\n",
       "              (1, 0.014317042685533674),\n",
       "              (3, 0.04363289199400738),\n",
       "              (7, 0.03272466899550554)],\n",
       "             101: [(0, 0.035546009259484375),\n",
       "              (1, 0.010830424696249145),\n",
       "              (2, 0.03300700859809263),\n",
       "              (3, 0.03300700859809263),\n",
       "              (7, 0.024755256448569473)],\n",
       "             102: [(7, 0.08223518189264449)],\n",
       "             103: [(5, 0.07939948596531193)],\n",
       "             104: [(1, 0.03597789207803197)],\n",
       "             105: [(0, 0.11808128682020748)],\n",
       "             106: [(4, 0.06977530584830442)],\n",
       "             107: [(4, 0.06977530584830442)],\n",
       "             108: [(1, 0.03597789207803197)],\n",
       "             109: [(1, 0.03597789207803197)],\n",
       "             110: [(3, 0.10964690919019265)],\n",
       "             111: [(8, 0.1151292546497023)],\n",
       "             112: [(1, 0.03597789207803197)],\n",
       "             113: [(4, 0.06977530584830442)],\n",
       "             114: [(8, 0.1151292546497023)],\n",
       "             115: [(8, 0.1151292546497023)],\n",
       "             116: [(1, 0.014317042685533674),\n",
       "              (3, 0.04363289199400738),\n",
       "              (5, 0.031596232133591556),\n",
       "              (7, 0.03272466899550554)],\n",
       "             117: [(8, 0.1151292546497023)],\n",
       "             118: [(1, 0.03597789207803197)],\n",
       "             119: [(1, 0.025147467381782817), (3, 0.0766399005921)],\n",
       "             120: [(2, 0.10964690919019265)],\n",
       "             121: [(6, 0.04598394035526001), (9, 0.20117973905426254)],\n",
       "             122: [(8, 0.1151292546497023)],\n",
       "             123: [(1, 0.1079336762340959)],\n",
       "             124: [(1, 0.03597789207803197)],\n",
       "             125: [(5, 0.07939948596531193)],\n",
       "             126: [(5, 0.07939948596531193)],\n",
       "             127: [(9, 0.28782313662425574)],\n",
       "             128: [(2, 0.10964690919019265)],\n",
       "             129: [(1, 0.03597789207803197)],\n",
       "             130: [(1, 0.03597789207803197)],\n",
       "             131: [(1, 0.03597789207803197)],\n",
       "             132: [(5, 0.07939948596531193)],\n",
       "             133: [(1, 0.025147467381782817), (4, 0.048770845831336375)],\n",
       "             134: [(6, 0.06578814551411559)],\n",
       "             135: [(4, 0.06977530584830442)],\n",
       "             136: [(6, 0.06578814551411559)],\n",
       "             137: [(1, 0.03597789207803197)],\n",
       "             138: [(6, 0.06578814551411559)],\n",
       "             139: [(9, 0.28782313662425574)],\n",
       "             140: [(0, 0.05904064341010374)],\n",
       "             141: [(1, 0.07195578415606393)],\n",
       "             142: [(4, 0.06977530584830442)],\n",
       "             143: [(7, 0.08223518189264449)],\n",
       "             144: [(7, 0.08223518189264449)],\n",
       "             145: [(4, 0.03648402437351322),\n",
       "              (6, 0.03439922298074103),\n",
       "              (7, 0.042999028725926286)],\n",
       "             146: [(1, 0.014317042685533674),\n",
       "              (2, 0.04363289199400738),\n",
       "              (3, 0.04363289199400738),\n",
       "              (5, 0.031596232133591556)],\n",
       "             147: [(1, 0.07195578415606393)],\n",
       "             148: [(1, 0.03597789207803197)],\n",
       "             149: [(8, 0.1151292546497023)],\n",
       "             150: [(0, 0.04126763878036154), (1, 0.025147467381782817)],\n",
       "             151: [(1, 0.03597789207803197)],\n",
       "             152: [(8, 0.1151292546497023)],\n",
       "             153: [(5, 0.07939948596531193)],\n",
       "             154: [(8, 0.1151292546497023)],\n",
       "             155: [(2, 0.2192938183803853)],\n",
       "             156: [(5, 0.07939948596531193)],\n",
       "             157: [(8, 0.2302585092994046)],\n",
       "             158: [(5, 0.07939948596531193)],\n",
       "             159: [(5, 0.07939948596531193)],\n",
       "             160: [(1, 0.03597789207803197)],\n",
       "             161: [(1, 0.03597789207803197)],\n",
       "             162: [(1, 0.03597789207803197)],\n",
       "             163: [(3, 0.10964690919019265)],\n",
       "             164: [(5, 0.07939948596531193)],\n",
       "             165: [(7, 0.08223518189264449)],\n",
       "             166: [(7, 0.08223518189264449)],\n",
       "             167: [(0, 0.09261329264045663),\n",
       "              (1, 0.018812075067592752),\n",
       "              (6, 0.03439922298074103)],\n",
       "             168: [(2, 0.10964690919019265)],\n",
       "             169: [(5, 0.05549785904945173), (7, 0.05747992544407501)],\n",
       "             170: [(1, 0.03597789207803197)],\n",
       "             171: [(8, 0.2302585092994046)],\n",
       "             172: [(6, 0.06578814551411559)],\n",
       "             173: [(0, 0.04126763878036154), (4, 0.048770845831336375)],\n",
       "             174: [(1, 0.03597789207803197)],\n",
       "             175: [(4, 0.06977530584830442)],\n",
       "             176: [(5, 0.07939948596531193)],\n",
       "             177: [(2, 0.10964690919019265)],\n",
       "             178: [(0, 0.05904064341010374)],\n",
       "             179: [(1, 0.03597789207803197)],\n",
       "             180: [(3, 0.2192938183803853)],\n",
       "             181: [(1, 0.018812075067592752),\n",
       "              (2, 0.17199611490370514),\n",
       "              (7, 0.042999028725926286)],\n",
       "             182: [(6, 0.06578814551411559)],\n",
       "             183: [(8, 0.1151292546497023)],\n",
       "             184: [(5, 0.15879897193062387)],\n",
       "             185: [(4, 0.06977530584830442)],\n",
       "             186: [(4, 0.06977530584830442)],\n",
       "             187: [(1, 0.025147467381782817), (7, 0.05747992544407501)],\n",
       "             188: [(7, 0.08223518189264449)],\n",
       "             189: [(6, 0.06578814551411559)],\n",
       "             190: [(2, 0.10964690919019265)],\n",
       "             191: [(7, 0.08223518189264449)],\n",
       "             192: [(6, 0.13157629102823118)],\n",
       "             193: [(1, 0.03597789207803197)]})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop, nlargest,heapify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "user_query=\"room bedroom\"\n",
    "#str(input())\n",
    "user_query=preprocessing_text(user_query)\n",
    "user_query_tfidf=np.ones(len(user_query))\n",
    "\n",
    "list_term_idx=[]\n",
    "#list of dataframes\n",
    "list_tf_idf=[]\n",
    "\n",
    "result_set=[]\n",
    "\n",
    "result_tf_idf_dic=defaultdict(list)\n",
    "for word in user_query:\n",
    "    #if word exist in the vocabulary\n",
    "    if word in vocabulary.keys():\n",
    "        list_term_idx.append(set(inverted_idx[vocabulary[word]]))\n",
    "        list_tf_idf.append((inverted_idx2[vocabulary[word]]))#[:,1])\n",
    "        #result_tf_idf_dic\n",
    "    else:\n",
    "        list_term_idx.append({'x'})\n",
    "        break\n",
    "result_set=list(set.intersection(*list_term_idx))\n",
    "if 'x' in result_set or not result_set:\n",
    "    result_set='No results! Try again!'\n",
    "\n",
    "tf_idf_dic=defaultdict(list)\n",
    "\n",
    "for tf_idf_1doc in list_tf_idf:\n",
    "    for tuple_pair in tf_idf_1doc:\n",
    "        if tuple_pair[0] in result_set:\n",
    "            tf_idf_dic[tuple_pair[0]].append(tuple_pair[1])\n",
    "\n",
    "print(result_set)\n",
    "#result_set=finalize_output(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_tuples(user_query_tfidf,tf_idf_dic):\n",
    "    cosine_sim_lst_tuples=[]\n",
    "    for key,value in tf_idf_dic.items():\n",
    "        tf_idf_val=cosine_similarity([user_query_tfidf],[value])[0][0]\n",
    "        cosine_sim_lst_tuples.append((tf_idf_val,key))\n",
    "    return cosine_sim_lst_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_lst_tuples=cosine_sim_tuples(user_query_tfidf,tf_idf_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heapify_tuples(cosine_sim_lst_tuples,k):\n",
    "    heap = []\n",
    "    for item in cosine_sim_lst_tuples:\n",
    "         heappush(heap, item)\n",
    "    return wanted_doc(nlargest(k,heap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wanted_doc(heap_k_docs):\n",
    "    wanted_doc_ids={}\n",
    "    for tup in (heap_k_docs):\n",
    "        wanted_doc_ids[tup[1]]=round(tup[0],2)\n",
    "    return wanted_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_doc_ids=heapify_tuples(cosine_sim_lst_tuples,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a, b):\n",
    "    cosine_similarity=(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "    return 1 - cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1.0, 3: 0.95, 7: 1.0}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wanted_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([7, 1, 3])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_set=wanted_doc_ids.keys()\n",
    "result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([7, 1, 3])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "\n",
    "for i,val in enumerate(result_set):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    df=df.append(pd.read_csv('data/doc_'+str(val)+'.tsv',sep='\\t',usecols=['description','title','city','url']\n",
    "                             ,encoding='ISO-8859-1',index_col=False))\n",
    "    df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['similarity']=wanted_doc_ids.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>city</th>\n",
       "      <th>url</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Friendly Private Room in ?Quiet Neighborhood</td>\n",
       "      <td>This is a beautiful bedroom with a queen size bed and closet. We do not have pets and the house is always clean. The bathroom is shared and supplies such as towels and shampoo are available. We are only some miles from Downtown, TCU, TCC, and Stockyards.</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>https://www.airbnb.com/rooms/18977363?location=Cleburne%2C%20TX</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unique Location! Alamo Heights - Designer Inspired</td>\n",
       "      <td>Stylish, fully remodeled home in upscale NW  Alamo Heights Area. \\n\\nAmazing location - House conveniently located in quiet street, with beautiful seasoned trees, prestigious neighborhood and very close to the airport, 281, 410 loop and down-town area. \\n\\nFeaturing an open floor plan, original hardwood floors, 3 bedrooms, 3 FULL bathrooms + an independent garden-TV room which can sleep 2 more\\n\\nEuropean inspired kitchen and top of the line decor. Driveway can park 4 cars.</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>https://www.airbnb.com/rooms/17481455?location=Cibolo%2C%20TX</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private Room Close to Campus</td>\n",
       "      <td>Private bedroom in a cute little home situated in the coveted Garden Acres neighborhood in Bryan. The bedroom has its own private access and its own private bathroom.</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>https://www.airbnb.com/rooms/11839729?location=College%20Station%2C%20TX</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0  Friendly Private Room in ?Quiet Neighborhood         \n",
       "0  Unique Location! Alamo Heights - Designer Inspired   \n",
       "0  Private Room Close to Campus                         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         description  \\\n",
       "0  This is a beautiful bedroom with a queen size bed and closet. We do not have pets and the house is always clean. The bathroom is shared and supplies such as towels and shampoo are available. We are only some miles from Downtown, TCU, TCC, and Stockyards.                                                                                                                                                                                                                                      \n",
       "0  Stylish, fully remodeled home in upscale NW  Alamo Heights Area. \\n\\nAmazing location - House conveniently located in quiet street, with beautiful seasoned trees, prestigious neighborhood and very close to the airport, 281, 410 loop and down-town area. \\n\\nFeaturing an open floor plan, original hardwood floors, 3 bedrooms, 3 FULL bathrooms + an independent garden-TV room which can sleep 2 more\\n\\nEuropean inspired kitchen and top of the line decor. Driveway can park 4 cars.   \n",
       "0  Private bedroom in a cute little home situated in the coveted Garden Acres neighborhood in Bryan. The bedroom has its own private access and its own private bathroom.                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "          city  \\\n",
       "0  Fort Worth    \n",
       "0  San Antonio   \n",
       "0  Bryan         \n",
       "\n",
       "                                                                        url  \\\n",
       "0  https://www.airbnb.com/rooms/18977363?location=Cleburne%2C%20TX            \n",
       "0  https://www.airbnb.com/rooms/17481455?location=Cibolo%2C%20TX              \n",
       "0  https://www.airbnb.com/rooms/11839729?location=College%20Station%2C%20TX   \n",
       "\n",
       "   similarity  \n",
       "0  1.00        \n",
       "0  1.00        \n",
       "0  0.95        "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title','description','city','url','similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
