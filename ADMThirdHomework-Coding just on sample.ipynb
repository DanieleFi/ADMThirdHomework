{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from heapq import heappush, heappop, nlargest,heapify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_data=pd.read_csv(\"Airbnb_Texas_Rentals.csv\",usecols=['average_rate_per_night', 'bedrooms_count', 'city',\n",
    "       'date_of_listing', 'description', 'latitude', 'longitude', 'title','url'],parse_dates=['date_of_listing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['average_rate_per_night', 'bedrooms_count', 'city', 'date_of_listing',\n",
       "       'description', 'latitude', 'longitude', 'title', 'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rate_per_night</th>\n",
       "      <th>bedrooms_count</th>\n",
       "      <th>city</th>\n",
       "      <th>date_of_listing</th>\n",
       "      <th>description</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$27</td>\n",
       "      <td>2</td>\n",
       "      <td>Humble</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>Welcome to stay in private room with queen bed...</td>\n",
       "      <td>30.020138</td>\n",
       "      <td>-95.293996</td>\n",
       "      <td>2 Private rooms/bathroom 10min from IAH airport</td>\n",
       "      <td>https://www.airbnb.com/rooms/18520444?location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  average_rate_per_night bedrooms_count    city date_of_listing  \\\n",
       "0                    $27              2  Humble      2016-05-01   \n",
       "\n",
       "                                         description   latitude  longitude  \\\n",
       "0  Welcome to stay in private room with queen bed...  30.020138 -95.293996   \n",
       "\n",
       "                                             title  \\\n",
       "0  2 Private rooms/bathroom 10min from IAH airport   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.airbnb.com/rooms/18520444?location...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18259, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_rate_per_night    28\n",
       "bedrooms_count             3\n",
       "city                       0\n",
       "date_of_listing            0\n",
       "description                2\n",
       "latitude                  34\n",
       "longitude                 34\n",
       "title                      3\n",
       "url                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null values of the dataset\n",
    "airbnb_data.isnull().sum()\n",
    "#average_rate_per_night -> replace NAN with 0, convert to int\n",
    "#bedrooms_count -> There are only 3 records so we decided to replace NAN with a category based on the desciption if it's possible. \n",
    "#description, latitude, longitude, title -> replace NAN to 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_rate_per_night            object\n",
       "bedrooms_count                    object\n",
       "city                              object\n",
       "date_of_listing           datetime64[ns]\n",
       "description                       object\n",
       "latitude                         float64\n",
       "longitude                        float64\n",
       "title                             object\n",
       "url                               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(airbnb_data):\n",
    "    \"\"\"\n",
    "    Method that removes nan values and imputes them\n",
    "    \n",
    "    Input: dataframe\n",
    "    Output: cleaned dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    #replace NAN with 0\n",
    "    airbnb_data.average_rate_per_night.replace(np.nan, '$0',inplace=True)\n",
    "    #convert to int and remove $\n",
    "    airbnb_data.average_rate_per_night=airbnb_data.average_rate_per_night.replace('[\\$]', '', regex=True).astype(int)\n",
    "\n",
    "    #replace NAN with'unknown'\n",
    "\n",
    "    airbnb_data.description.replace(np.nan,'unknown',inplace=True)\n",
    "    airbnb_data.title.replace(np.nan,'unknown',inplace=True)\n",
    "\n",
    "    airbnb_data.latitude.replace(np.nan,'unknown',inplace=True)\n",
    "    airbnb_data.longitude.replace(np.nan,'unknown',inplace=True)\n",
    "\n",
    "    #check where bedrooms_count doesn't have a value and save indexes of those records to a list\n",
    "    null_value_idx=airbnb_data[airbnb_data.bedrooms_count.isnull()].index\n",
    "    #if the word studio is mentioned in description then it is a studio otherwise 'unknown'\n",
    "    for idx in null_value_idx:\n",
    "        if 'studio' in airbnb_data.iloc[idx].description.split():\n",
    "            airbnb_data.bedrooms_count[idx]='Studio'\n",
    "        else:\n",
    "            airbnb_data.bedrooms_count[idx]='unknown'\n",
    "        \n",
    "    return airbnb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "average_rate_per_night    0\n",
       "bedrooms_count            0\n",
       "city                      0\n",
       "date_of_listing           0\n",
       "description               0\n",
       "latitude                  0\n",
       "longitude                 0\n",
       "title                     0\n",
       "url                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data=clean(airbnb_data)\n",
    "airbnb_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18259, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tsv_documents(airbnb_data):\n",
    "    \"\"\"\n",
    "    Method that creates different .tsv files for each record in the airbnb_data \n",
    "    \n",
    "    Input: dataframe\n",
    "    \"\"\"   \n",
    "    #clean data\n",
    "    airbnb_data=clean(airbnb_data)\n",
    "    \n",
    "    #for each index make a dataframe of airbnb_data and store it into new tsv file\n",
    "    for i in airbnb_data.index:\n",
    "        pd.DataFrame(airbnb_data.loc[i]).transpose().to_csv('data/doc_'+str(i)+'.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#method is run only once at the beginning to make separate .tsv files\n",
    "create_tsv_documents(airbnb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "1) Removing stop words\n",
    "\n",
    "2) Removing punctuation\n",
    "\n",
    "3) Stemming\n",
    "\n",
    "##### remove non english words and words Giulia chooses (room, price, airbnb) MOST often ones_?\n",
    "##### should we remove numbers__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1) Conjunctive query\n",
    "\n",
    "## 3.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(df):\n",
    "    #remove upper cases\n",
    "    df=df.lower()\n",
    "    #replacing new line sign '\\n' with a whitespace ' '    \n",
    "    df=df.replace('\\\\n',' ')\n",
    "\n",
    "    #removing stop words and punctuation\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    #for removing punctuations\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    #to tokenize the string\n",
    "    word_tokens = tokenizer.tokenize(df) \n",
    "\n",
    "    #stemming\n",
    "    ps = PorterStemmer()\n",
    "    filtered_words = [ps.stem(w) for w in word_tokens if not w in stop_words] \n",
    "\n",
    "    #remove non-english words\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a vocabulary\n",
    "\n",
    "#set for vocabulary (values of the set will be the keys fo vocabulary_dict)\n",
    "vocabulary_lst=[]\n",
    "#building a dictionary which will be used for making an inverted index\n",
    "doc_vocabs=defaultdict(list)\n",
    "\n",
    "for i in range(sample):\n",
    "    #take one file\n",
    "    df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "    #preprocessing \n",
    "    df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "    filtered_words=preprocessing_text(df)\n",
    "    temp_vocabulary_set=set()\n",
    "    for word in filtered_words:\n",
    "        temp_vocabulary_set.add(word)\n",
    "    vocabulary_lst.append(temp_vocabulary_set)\n",
    "    doc_vocabs[i]=list(temp_vocabulary_set)\n",
    "vocabulary_set=set.union(*vocabulary_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary={}\n",
    "for k,v in enumerate(vocabulary_set):\n",
    "    #just for testing\n",
    "    #vocabulary[v]='id'+str(k)\n",
    "    vocabulary[v]= k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(vocabulary): \n",
    "    \"\"\"\n",
    "    method that converts vocabulary into a dataframe and saves it into a csv file\n",
    "    \n",
    "    input: vocabulary(dictionary, key='term',value='term_id')\n",
    "    \"\"\"\n",
    "    vocabulary_dataframe=pd.DataFrame()\n",
    "    vocabulary_dataframe['word']=vocabulary.keys()\n",
    "    vocabulary_dataframe.to_csv('vocabulary_sample.csv')\n",
    "    del vocabulary_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocabulary(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute an inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inverted_idx(doc_vocabs,vocabulary):\n",
    "    \"\"\"\n",
    "    method that computes an inverted index\n",
    "    \n",
    "    input: doc_vocabs(dictionary), vocabulary(dictionary of all unique words, key=term, value=term_id)\n",
    "    output: inverted_idx(dictionary, key=term_id, value=list of document_ids) \n",
    "    \"\"\"\n",
    "    #initialize defaultdict for making an inverted index\n",
    "    inverted_idx = defaultdict(list)\n",
    "    #in every document look for every word and assign document id to the words which belong to it\n",
    "    for idx in doc_vocabs.keys():\n",
    "        for word in doc_vocabs[idx]:\n",
    "            inverted_idx[vocabulary[word]].append(idx)\n",
    "    return inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx=compute_inverted_idx(doc_vocabs,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a dictionary into a pickle file.\n",
    "import pickle\n",
    "\n",
    "pickle.dump(inverted_idx, open(\"inv_idx_sample.p\", \"wb\"))  # save it into a file named save.p\n",
    "\n",
    "# Load the dictionary back from the pickle file.\n",
    "\n",
    "inverted_index = pickle.load(open(\"inv_idx_sample.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_output(result_set):\n",
    "    df=pd.DataFrame()\n",
    "    for i,val in enumerate(result_set):\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "        df=df.append(pd.read_csv('data/doc_'+str(val)+'.tsv',sep='\\t',usecols=['description','title','city','url']\n",
    "                                 ,encoding='ISO-8859-1',index_col=False))\n",
    "        df.reset_index().drop('index',axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine():\n",
    "    user_query=str(input())\n",
    "    \n",
    "    user_query=preprocessing_text(user_query)\n",
    "\n",
    "    list_term_idx=[]\n",
    "    result_set=[]\n",
    "    for word in user_query:\n",
    "        #if word exist in the vocabulary\n",
    "        if word in vocabulary.keys():\n",
    "            list_term_idx.append(set(inverted_idx[vocabulary[word]]))\n",
    "        else:\n",
    "            list_term_idx.append({'x'})\n",
    "            break\n",
    "    result_set=list(set.intersection(*list_term_idx))\n",
    "    if 'x' in result_set or not result_set:\n",
    "        result_set='No results! Try again!'\n",
    "        return result_set\n",
    "        \n",
    "    print(result_set)\n",
    "    result_set=finalize_output(result_set)\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2) Conjunctive query & Ranking score\n",
    "### 3.2.1) Inverted index\n",
    "### 3.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate tfldf\n",
    "\n",
    "1) tf=term frequency -- the frequency of the word in each document in the corpus.\n",
    "\n",
    "2) idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc tfidf and cosine similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "Total_number_of_documents=airbnb_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "tf_dic=dict()\n",
    "for i in range(sample):\n",
    "    #take one file\n",
    "    df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "    #preprocessing \n",
    "    df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "    filtered_words=preprocessing_text(df)\n",
    "    tf_series=pd.Series(filtered_words)\n",
    "    tf_series=(tf_series.value_counts())/len(tf_series)\n",
    "    for index,value in tf_series.iteritems():\n",
    "        tf_dic[index,i]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "idf_dic=dict()\n",
    "total_num_docs_sample=sample\n",
    "for i in range(sample):\n",
    "    #take one file\n",
    "    df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "    #preprocessing \n",
    "    df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "    \n",
    "    filtered_words=preprocessing_text(df)\n",
    " #   for word_k in filtered_words_2: \n",
    "    idf_series=pd.Series(list(set(filtered_words)))\n",
    "    idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])) )\n",
    "    for idx in range(len(idf_series)):\n",
    "        idf_dic[idf_series[idx],i]=idf_calc[idx]      \n",
    "        \n",
    "  #      lis.append(np.log(total_num_docs_sample/len(inverted_idx[vocabulary[word_k]])))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "#preprocessing \n",
    "df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "filtered_words=preprocessing_text(df)\n",
    "\n",
    "tf_series=pd.Series(filtered_words)\n",
    "tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "#idf_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf_calc.sort_index()\n",
    "result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),idf_calc],axis=1)#.reset_index()\n",
    "result_df['res']=result_df[1]*result_df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antonio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cozi</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entranc</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>histor</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.916291</td>\n",
       "      <td>0.114536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>privat</td>\n",
       "      <td>0.250</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.575646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>san</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>studio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.203973</td>\n",
       "      <td>0.150497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1         2       res\n",
       "0  antonio  0.125  1.609438  0.201180\n",
       "1     cozi  0.125  1.609438  0.201180\n",
       "2  entranc  0.125  2.302585  0.287823\n",
       "3   histor  0.125  0.916291  0.114536\n",
       "4   privat  0.250  2.302585  0.575646\n",
       "5      san  0.125  2.302585  0.287823\n",
       "6   studio  0.125  1.203973  0.150497"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antonio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cozi</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entranc</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>histor</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.916291</td>\n",
       "      <td>0.114536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>privat</td>\n",
       "      <td>0.250</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.575646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>san</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>studio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.203973</td>\n",
       "      <td>0.150497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1         2       res\n",
       "0  antonio  0.125  1.609438  0.201180\n",
       "1     cozi  0.125  1.609438  0.201180\n",
       "2  entranc  0.125  2.302585  0.287823\n",
       "3   histor  0.125  0.916291  0.114536\n",
       "4   privat  0.250  2.302585  0.575646\n",
       "5      san  0.125  2.302585  0.287823\n",
       "6   studio  0.125  1.203973  0.150497"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "tf_idf_dic=dict()\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "idf_dic2={}\n",
    "tf_dic2={}\n",
    "proba={}\n",
    "#OVDE\n",
    "total_num_docs_sample=sample\n",
    "result_df=pd.DataFrame()\n",
    "for i in range(sample):\n",
    "    #take one file\n",
    "    df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "    #preprocessing \n",
    "    df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "    filtered_words=preprocessing_text(df)\n",
    "    tf_series=pd.Series(filtered_words)\n",
    "    tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "    idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "    idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "    #idf_calc.sort_index()\n",
    "    result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),pd.Series(idf_calc.values)],axis=1)#.reset_index()\n",
    "#    result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),idf_calc],axis=1)#.reset_index()\n",
    "    result_df['tf_idf']=result_df[1]*result_df[2]\n",
    "   # result_df=result_df.loc[:,['res',0]]\n",
    "    \n",
    "    for idx in range(result_df.shape[0]):\n",
    "        tf_idf_dic[result_df[0][idx],i]=result_df['tf_idf'][idx]\n",
    "#del result_df\n",
    "    for idx in range(len(tf_series)):\n",
    "#        tf_idf_dic[tf_series.index[idx],i]=tf_series[idx]*idf_calc[idx]\n",
    "        idf_dic2[idf_series[idx],i]=idf_calc[idx] \n",
    "    for index,value in tf_series.iteritems():\n",
    "        tf_dic2[index,i]=value\n",
    "    for k in tf_dic2.keys():\n",
    "        proba[k]=tf_dic2[k]*idf_dic2[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antonio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cozi</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.201180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entranc</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>histor</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>privat</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.916291</td>\n",
       "      <td>0.229073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>san</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.203973</td>\n",
       "      <td>0.150497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>studio</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.287823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1         2    tf_idf\n",
       "0  antonio  0.125  1.609438  0.201180\n",
       "1     cozi  0.125  1.609438  0.201180\n",
       "2  entranc  0.125  2.302585  0.287823\n",
       "3   histor  0.125  2.302585  0.287823\n",
       "4   privat  0.250  0.916291  0.229073\n",
       "5      san  0.125  1.203973  0.150497\n",
       "6   studio  0.125  2.302585  0.287823"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.60943791, 1.60943791, 2.30258509, 2.30258509, 0.91629073,\n",
       "       1.2039728 , 2.30258509])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_calc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tf_dic.keys():\n",
    "    if(tf_dic[key]*idf_dic[key]!=proba[key]):\n",
    " #       print(key,tf_dic[key])\n",
    "#        print(key,idf_dic[key])\n",
    "        print(key,tf_dic[key]*idf_dic[key])\n",
    "        print(key,tf_idf_dic[key])\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05733203830123505"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba['san',2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tf_dic.keys():\n",
    "    if(tf_dic[key]*idf_dic[key]!=tf_idf_dic[key]):\n",
    " #       print(key,tf_dic[key])\n",
    "#        print(key,idf_dic[key])\n",
    "        print(key,tf_dic[key]*idf_dic[key])\n",
    "        print(key,tf_idf_dic[key])\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First way\n",
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "def calculate_tf_idf(airbnb_data):\n",
    "    tf_idf_dic=dict()\n",
    "    total_num_docs_sample=sample\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "        result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),pd.Series(idf_calc.values)],axis=1)#.reset_index()\n",
    "        result_df['tf_idf']=result_df[1]*result_df[2]\n",
    "\n",
    "        for idx in range(result_df.shape[0]):\n",
    "            tf_idf_dic[result_df[0][idx],i]=result_df['tf_idf'][idx]\n",
    "    return tf_idf_dic        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second way--to check if it is the same like the 1st-for double checking the results\n",
    "def calculate_tf_idf2(airbnb_data):\n",
    "    idf_dic2={}\n",
    "    tf_dic2={}\n",
    "    proba={}\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "       \n",
    "        for idx in range(len(tf_series)):\n",
    "            idf_dic2[idf_series[idx],i]=idf_calc[idx] \n",
    "        for index,value in tf_series.iteritems():\n",
    "            tf_dic2[index,i]=value\n",
    "        for k in tf_dic2.keys():\n",
    "            proba[k]=tf_dic2[k]*idf_dic2[k]\n",
    "    return proba        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=calculate_tf_idf(airbnb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=calculate_tf_idf2(airbnb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in a.keys():\n",
    "    if(a[k]!=b[k]):\n",
    "        print('њет')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inverted_idx(doc_vocabs,vocabulary):\n",
    "    \"\"\"\n",
    "    method that computes an inverted index\n",
    "    \n",
    "    input: doc_vocabs(dictionary), vocabulary(dictionary of all unique words, key=term, value=term_id)\n",
    "    output: inverted_idx(dictionary, key=term_id, value=list of document_ids) \n",
    "    \"\"\"\n",
    "    #initialize defaultdict for making an inverted index\n",
    "    inverted_idx = defaultdict(list)\n",
    "    #in every document look for every word and assign document id to the words which belong to it\n",
    "    for idx in doc_vocabs.keys():\n",
    "        for word in doc_vocabs[idx]:\n",
    "            inverted_idx[vocabulary[word]].append(idx)\n",
    "    return inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx=compute_inverted_idx(doc_vocabs,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=('10', 0)\n",
    "aa=aa[1]\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70710678],\n",
       "       [0.70710678, 1.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=[1,2]\n",
    "Y=[3,1]\n",
    "cosine_similarity([X,Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2=defaultdict(list)\n",
    "for k,v in a.items():\n",
    "    k=k[1]\n",
    "    a2[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [0.11808128682020748,\n",
       "              0.11808128682020748,\n",
       "              0.04126763878036154,\n",
       "              0.11808128682020748,\n",
       "              0.09261329264045663,\n",
       "              0.05904064341010374,\n",
       "              0.06174219509363775,\n",
       "              0.04698926830123872,\n",
       "              0.08253527756072308,\n",
       "              0.017773004629742187,\n",
       "              0.05904064341010374,\n",
       "              0.05904064341010374,\n",
       "              0.04126763878036154,\n",
       "              0.11808128682020748,\n",
       "              0.05904064341010374,\n",
       "              0.11808128682020748,\n",
       "              0.05904064341010374,\n",
       "              0.09397853660247744,\n",
       "              0.04126763878036154,\n",
       "              0.035546009259484375,\n",
       "              0.05904064341010374,\n",
       "              0.05904064341010374,\n",
       "              0.05904064341010374,\n",
       "              0.05904064341010374,\n",
       "              0.04126763878036154],\n",
       "             1: [0.025147467381782817,\n",
       "              0.03597789207803197,\n",
       "              0.07195578415606393,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.018812075067592752,\n",
       "              0.07195578415606393,\n",
       "              0.03597789207803197,\n",
       "              0.025147467381782817,\n",
       "              0.07195578415606393,\n",
       "              0.014317042685533674,\n",
       "              0.025147467381782817,\n",
       "              0.010830424696249145,\n",
       "              0.03597789207803197,\n",
       "              0.014317042685533674,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.050294934763565634,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.025147467381782817,\n",
       "              0.03597789207803197,\n",
       "              0.07195578415606393,\n",
       "              0.010830424696249145,\n",
       "              0.018812075067592752,\n",
       "              0.03597789207803197,\n",
       "              0.07195578415606393,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.1079336762340959,\n",
       "              0.03597789207803197,\n",
       "              0.014317042685533674,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.025147467381782817,\n",
       "              0.018812075067592752,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.025147467381782817,\n",
       "              0.03597789207803197,\n",
       "              0.010830424696249145,\n",
       "              0.018812075067592752,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.025147467381782817,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197,\n",
       "              0.03597789207803197],\n",
       "             2: [0.2192938183803853,\n",
       "              0.04363289199400738,\n",
       "              0.10964690919019265,\n",
       "              0.17199611490370514,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.4385876367607706,\n",
       "              0.03300700859809263,\n",
       "              0.05733203830123505,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265],\n",
       "             3: [0.10964690919019265,\n",
       "              0.0766399005921,\n",
       "              0.04363289199400738,\n",
       "              0.06601401719618526,\n",
       "              0.2192938183803853,\n",
       "              0.10964690919019265,\n",
       "              0.04363289199400738,\n",
       "              0.10964690919019265,\n",
       "              0.10964690919019265,\n",
       "              0.0766399005921,\n",
       "              0.03300700859809263,\n",
       "              0.10964690919019265,\n",
       "              0.04363289199400738,\n",
       "              0.17453156797602953,\n",
       "              0.03300700859809263,\n",
       "              0.10964690919019265],\n",
       "             4: [0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.09754169166267275,\n",
       "              0.06977530584830442,\n",
       "              0.09754169166267275,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06301338005090412,\n",
       "              0.03648402437351322,\n",
       "              0.06977530584830442,\n",
       "              0.048770845831336375,\n",
       "              0.09754169166267275,\n",
       "              0.06977530584830442,\n",
       "              0.03648402437351322,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.06977530584830442,\n",
       "              0.048770845831336375,\n",
       "              0.048770845831336375,\n",
       "              0.09754169166267275],\n",
       "             5: [0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.031596232133591556,\n",
       "              0.15879897193062387,\n",
       "              0.05549785904945173,\n",
       "              0.07939948596531193,\n",
       "              0.15879897193062387,\n",
       "              0.05549785904945173,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.02390162691586018,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.031596232133591556,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.23819845789593577,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193,\n",
       "              0.07939948596531193],\n",
       "             6: [0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.04598394035526001,\n",
       "              0.06578814551411559,\n",
       "              0.03439922298074103,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.13157629102823118,\n",
       "              0.03439922298074103,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.09196788071052002,\n",
       "              0.04598394035526001,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.01980420515885558,\n",
       "              0.06578814551411559,\n",
       "              0.03439922298074103,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.04598394035526001,\n",
       "              0.06578814551411559,\n",
       "              0.03439922298074103,\n",
       "              0.06578814551411559,\n",
       "              0.13157629102823118,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559,\n",
       "              0.06578814551411559],\n",
       "             7: [0.08223518189264449,\n",
       "              0.042999028725926286,\n",
       "              0.03272466899550554,\n",
       "              0.05747992544407501,\n",
       "              0.05747992544407501,\n",
       "              0.024755256448569473,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.05747992544407501,\n",
       "              0.05747992544407501,\n",
       "              0.05747992544407501,\n",
       "              0.042999028725926286,\n",
       "              0.042999028725926286,\n",
       "              0.03272466899550554,\n",
       "              0.08223518189264449,\n",
       "              0.03272466899550554,\n",
       "              0.05747992544407501,\n",
       "              0.05747992544407501,\n",
       "              0.024755256448569473,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.08223518189264449,\n",
       "              0.05747992544407501],\n",
       "             8: [0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.03465735902799726,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.2302585092994046,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.2302585092994046,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023,\n",
       "              0.1151292546497023],\n",
       "             9: [0.20117973905426254,\n",
       "              0.20117973905426254,\n",
       "              0.28782313662425574,\n",
       "              0.28782313662425574,\n",
       "              0.22907268296853878,\n",
       "              0.15049660054074201,\n",
       "              0.28782313662425574]})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(0.92,0.3200323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spatial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-faa2dad44dc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#TFIDF_query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.1446624\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m0.72522043\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.74339473\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spatial' is not defined"
     ]
    }
   ],
   "source": [
    "a=[0.05215743696136418,\n",
    "  0.03257057712576686,\n",
    "  0.032268990804419175]\n",
    "#TFIDF_query\n",
    "b=np.array([1.1446624 , 0.72522043, 0.74339473])\n",
    "1 - spatial.distance.cosine(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "dataSetI = [0.03597789207803197\n",
    "           # ,0.03597789207803197,\n",
    "            #  0.03597789207803197,\n",
    "            #  0.03597789207803197\n",
    "           ]\n",
    "dataSetII = [0.03597789207803197#,\n",
    "             # 0.03597789207803197,\n",
    "             # 1.03597789207803197,\n",
    "             # 2.03597789207803197\n",
    "            ]\n",
    "result = 1 - spatial.distance.cosine(dataSetI, dataSetII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2=defaultdict(tuple)\n",
    "for term_id in inverted_idx.keys():\n",
    "    for k,v in vocabulary.items():\n",
    "        if v==term_id:\n",
    "            term=k\n",
    "    for doc_id in range(len(inverted_idx[term_id])):\n",
    "        inverted_idx[term_id].append(\n",
    "            (doc_id,\n",
    "             a[term,doc_id]))\n",
    "        inverted_idx[term_id].remove(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id in inverted_idx[term_id]:\n",
    "    print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a dictionary into a pickle file.\n",
    "import pickle\n",
    "\n",
    "pickle.dump(inverted_idx, open(\"inv_idx2_sample.p\", \"wb\"))  # save it into a file named save.p\n",
    "\n",
    "# Load the dictionary back from the pickle file.\n",
    "\n",
    "inverted_index = pickle.load(open(\"inv_idx2_sample.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INVERTED INDEX 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic=calculate_tf_idf(airbnb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2=defaultdict(list)\n",
    "for term_id in inverted_idx.keys():\n",
    "    for k,v in vocabulary.items():#k->term, v->term_id\n",
    "        if v==term_id:\n",
    "            term=k\n",
    "    for doc_id in inverted_idx[term_id]:\n",
    "        inverted_idx2[term_id].append((doc_id,tf_idf_dic[term,doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary['airport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index[vocabulary['bathroom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverted_idx2[vocabulary['bathroom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query=\"room bedroom\"\n",
    "#str(input())\n",
    "user_query=preprocessing_text(user_query)\n",
    "user_query_tfidf=np.ones(len(user_query))\n",
    "\n",
    "\n",
    "list_term_idx=[]\n",
    "#list of dataframes\n",
    "list_tf_idf=[]\n",
    "\n",
    "result_set=[]\n",
    "\n",
    "result_tf_idf_dic=defaultdict(list)\n",
    "for word in user_query:\n",
    "    #if word exist in the vocabulary\n",
    "    if word in vocabulary.keys():\n",
    "        list_term_idx.append(set(inverted_idx[vocabulary[word]]))\n",
    "        list_tf_idf.append((inverted_idx2[vocabulary[word]]))#[:,1])\n",
    "        #result_tf_idf_dic\n",
    "    else:\n",
    "        list_term_idx.append({'x'})\n",
    "        break\n",
    "result_set=list(set.intersection(*list_term_idx))\n",
    "if 'x' in result_set or not result_set:\n",
    "    result_set='No results! Try again!'\n",
    "\n",
    "tf_idf_dic=defaultdict(list)\n",
    "\n",
    "for tf_idf_1doc in list_tf_idf:\n",
    "    for tuple_pair in tf_idf_1doc:\n",
    "        if tuple_pair[0] in result_set:\n",
    "            tf_idf_dic[tuple_pair[0]].append(tuple_pair[1])\n",
    "\n",
    "print(result_set)\n",
    "#result_set=finalize_output(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst=[]\n",
    "cosine_similarity_dic={}\n",
    "for value in tf_idf_dic.values():\n",
    "    print(cosine_similarity([user_query_tfidf],[value]))\n",
    "    cosine_similarity_lst.append(cosine_similarity([user_query_tfidf],[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop, nlargest,heapify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap = []\n",
    "for item in cosine_similarity_lst:\n",
    "     heappush(heap, item)\n",
    "#heapify(cosine_similarity_lst)\n",
    "#ordered = []\n",
    "#while heap:\n",
    "#    ordered.append(heappop(heap))\n",
    "#ordered\n",
    "#heap.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlargest(3,heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst.hea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-cosine_similarity([np.array(a)],[np.array(b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a, b):\n",
    "    cosine_similarity=(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "    return 1 - cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distance([1,1,1],[0.03554600925948437,0.5,0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2[14][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query.index('rooma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.DataFrame(inverted_idx2[vocabulary['room']],columns=['doc_id','tf_idf'+str(user_query.index(word))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=pd.DataFrame(inverted_idx2[vocabulary['bedroom']],columns=['doc_id','tf_idf'+str(user_query.index(word))])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(*c,left_on='doc_id',right_on='doc_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(inverted_idx2[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_term_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_output(result_set):\n",
    "    df=pd.DataFrame()\n",
    "    for i,val in enumerate(result_set):\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "        df=df.append(pd.read_csv('data/doc_'+str(val)+'.tsv',sep='\\t',usecols=['description','title','city','url']\n",
    "                                 ,encoding='ISO-8859-1',index_col=False))\n",
    "        df.reset_index().drop('index',axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine():\n",
    "    user_query=str(input())\n",
    "    \n",
    "    user_query=preprocessing_text(user_query)\n",
    "\n",
    "    list_term_idx=[]\n",
    "    result_set=[]\n",
    "    for word in user_query:\n",
    "        #if word exist in the vocabulary\n",
    "        if word in vocabulary.keys():\n",
    "            list_term_idx.append(set(inverted_idx[vocabulary[word]]))\n",
    "        else:\n",
    "            list_term_idx.append({'x'})\n",
    "            break\n",
    "    result_set=list(set.intersection(*list_term_idx))\n",
    "    if 'x' in result_set or not result_set:\n",
    "        result_set='No results! Try again!'\n",
    "        return result_set\n",
    "        \n",
    "    print(result_set)\n",
    "    result_set=finalize_output(result_set)\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more concise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2) Conjunctive query & Ranking score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1) Inverted index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First way\n",
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "def calculate_tf_idf(airbnb_data):\n",
    "    tf_idf_dic=dict()\n",
    "    total_num_docs_sample=sample\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "        result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),pd.Series(idf_calc.values)],axis=1)#.reset_index()\n",
    "        result_df['tf_idf']=result_df[1]*result_df[2]\n",
    "\n",
    "        for idx in range(result_df.shape[0]):\n",
    "            tf_idf_dic[result_df[0][idx],i]=result_df['tf_idf'][idx]\n",
    "    return tf_idf_dic        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second way--to check if it is the same like the 1st-for double checking the results\n",
    "def calculate_tf_idf2(airbnb_data):\n",
    "    idf_dic2={}\n",
    "    tf_dic2={}\n",
    "    proba={}\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "       \n",
    "        for idx in range(len(tf_series)):\n",
    "            idf_dic2[idf_series[idx],i]=idf_calc[idx] \n",
    "        for index,value in tf_series.iteritems():\n",
    "            tf_dic2[index,i]=value\n",
    "        for k in tf_dic2.keys():\n",
    "            proba[k]=tf_dic2[k]*idf_dic2[k]\n",
    "    return proba        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic=calculate_tf_idf(airbnb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2=defaultdict(list)\n",
    "for term_id in inverted_idx.keys():\n",
    "    for k,v in vocabulary.items():#k->term, v->term_id\n",
    "        if v==term_id:\n",
    "            term=k\n",
    "    for doc_id in inverted_idx[term_id]:\n",
    "        inverted_idx2[term_id].append((doc_id,tf_idf_dic[term,doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverted_idx2[vocabulary['bathroom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst=[]\n",
    "for value in tf_idf_dic.values():\n",
    "    tf_idf_val=cosine_similarity([user_query_tfidf],[value])[0][0]\n",
    "    #keys()\n",
    "    cosine_similarity_lst.append(tf_idf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop, nlargest,heapify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst_tuples=[]\n",
    "for key,value in tf_idf_dic.items():\n",
    "    tf_idf_val=cosine_similarity([user_query_tfidf],[value])[0][0]\n",
    "    cosine_similarity_lst_tuples.append((tf_idf_val,key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_lst_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a, b):\n",
    "    cosine_similarity=(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "    return 1 - cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distance([1,1,1],[0.03554600925948437,0.5,0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap = []\n",
    "for item in cosine_similarity_lst_tuples:\n",
    "     heappush(heap, item)\n",
    "#heap.sort(reverse=True)\n",
    "nlargest(3,heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(nlargest(3,heap))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query=\"room bedroom\"\n",
    "#str(input())\n",
    "user_query=preprocessing_text(user_query)\n",
    "user_query_tfidf=np.ones(len(user_query))\n",
    "\n",
    "list_term_idx=[]\n",
    "#list of dataframes\n",
    "list_tf_idf=[]\n",
    "\n",
    "result_set=[]\n",
    "\n",
    "result_tf_idf_dic=defaultdict(list)\n",
    "for word in user_query:\n",
    "    #if word exist in the vocabulary\n",
    "    if word in vocabulary.keys():\n",
    "        list_term_idx.append(set(inverted_idx[vocabulary[word]]))\n",
    "        list_tf_idf.append((inverted_idx2[vocabulary[word]]))#[:,1])\n",
    "        #result_tf_idf_dic\n",
    "    else:\n",
    "        list_term_idx.append({'x'})\n",
    "        break\n",
    "result_set=list(set.intersection(*list_term_idx))\n",
    "if 'x' in result_set or not result_set:\n",
    "    result_set='No results! Try again!'\n",
    "\n",
    "tf_idf_dic=defaultdict(list)\n",
    "\n",
    "for tf_idf_1doc in list_tf_idf:\n",
    "    for tuple_pair in tf_idf_1doc:\n",
    "        if tuple_pair[0] in result_set:\n",
    "            tf_idf_dic[tuple_pair[0]].append(tuple_pair[1])\n",
    "\n",
    "print(result_set)\n",
    "#result_set=finalize_output(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_doc_ids={}\n",
    "for tup in (nlargest(3,heap)):\n",
    "    wanted_doc_ids[tup[1]]=tup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_set=wanted_doc_ids.keys()\n",
    "result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "\n",
    "for i,val in enumerate(result_set):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    df=df.append(pd.read_csv('data/doc_'+str(val)+'.tsv',sep='\\t',usecols=['description','title','city','url']\n",
    "                             ,encoding='ISO-8859-1',index_col=False))\n",
    "    df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['similarity']=wanted_doc_ids.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['title','description','city','url','similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sort_values('Similarity',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2) Conjunctive query & Ranking score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1) Inverted index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First way\n",
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "def calculate_tf_idf(airbnb_data):\n",
    "    tf_idf_dic=dict()\n",
    "    total_num_docs_sample=sample\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "        result_df=pd.concat([pd.Series(idf_series.values),pd.Series(tf_series.values),pd.Series(idf_calc.values)],axis=1)#.reset_index()\n",
    "        result_df['tf_idf']=result_df[1]*result_df[2]\n",
    "\n",
    "        for idx in range(result_df.shape[0]):\n",
    "            tf_idf_dic[result_df[0][idx],i]=result_df['tf_idf'][idx]\n",
    "    return tf_idf_dic        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second way--to check if it is the same like the 1st-for double checking the results\n",
    "def calculate_tf_idf2(airbnb_data):\n",
    "    idf_dic2={}\n",
    "    tf_dic2={}\n",
    "    proba={}\n",
    "    for i in range(sample):\n",
    "        #take one file\n",
    "        df=pd.read_csv('data/doc_'+str(i)+'.tsv',sep='\\t',usecols=['description','title','city'],encoding='ISO-8859-1')\n",
    "        #preprocessing \n",
    "        df=df.description[0]+' '+df.title[0]+' '+df.city[0]\n",
    "        filtered_words=preprocessing_text(df)\n",
    "        tf_series=pd.Series(filtered_words)\n",
    "        tf_series=((tf_series.value_counts())/len(tf_series)).sort_index()\n",
    "        idf_series=pd.Series(list(set(filtered_words))).sort_values()\n",
    "        idf_calc=idf_series.apply(lambda x: np.log(total_num_docs_sample/len(inverted_idx[vocabulary[x]])))\n",
    "       \n",
    "        for idx in range(len(tf_series)):\n",
    "            idf_dic2[idf_series[idx],i]=idf_calc[idx] \n",
    "        for index,value in tf_series.iteritems():\n",
    "            tf_dic2[index,i]=value\n",
    "        for k in tf_dic2.keys():\n",
    "            proba[k]=tf_dic2[k]*idf_dic2[k]\n",
    "    return proba        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inverted_idx2(inverted_idx,vocabulary,tf_idf_dic):\n",
    "    \"\"\"\n",
    "    method that computes the second inverted index\n",
    "    \n",
    "    input:  \n",
    "    output: inverted_idx2(dictionary, key=term_id, value=list of tuples (document_id,tf_idf value) )\n",
    "    \"\"\"\n",
    "    inverted_idx2=defaultdict(list)\n",
    "    for term_id in inverted_idx.keys():\n",
    "        for k,v in vocabulary.items():#k->term, v->term_id\n",
    "            if v==term_id:\n",
    "                term=k\n",
    "        for doc_id in inverted_idx[term_id]:\n",
    "            inverted_idx2[term_id].append((doc_id,tf_idf_dic[term,doc_id]))\n",
    "    return inverted_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic=calculate_tf_idf(airbnb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx2=compute_inverted_idx2(inverted_idx,vocabulary,tf_idf_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.03597789207803197)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_idx2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine(k):\n",
    "    user_query=str(input())\n",
    "    user_query=preprocessing_text(user_query)\n",
    "    user_query_tfidf=np.ones(len(user_query))\n",
    "\n",
    "    list_term_idx=[]\n",
    "    #list of dataframes\n",
    "    list_tf_idf=[]\n",
    "\n",
    "    result_set=[]\n",
    "\n",
    "    result_tf_idf_dic=defaultdict(list)\n",
    "    for word in user_query:\n",
    "        #if word exist in the vocabulary\n",
    "        if word in vocabulary.keys():\n",
    "            list_term_idx.append(set(inverted_idx[vocabulary[word]]))\n",
    "            list_tf_idf.append((inverted_idx2[vocabulary[word]]))#[:,1])\n",
    "            #result_tf_idf_dic\n",
    "        else:\n",
    "            list_term_idx.append({'x'})\n",
    "            break\n",
    "    result_set=list(set.intersection(*list_term_idx))\n",
    "    if 'x' in result_set or not result_set:\n",
    "        result_set='No results! Try again!'\n",
    "\n",
    "    tf_idf_dic=defaultdict(list)\n",
    "\n",
    "    for tf_idf_1doc in list_tf_idf:\n",
    "        for tuple_pair in tf_idf_1doc:\n",
    "            if tuple_pair[0] in result_set:\n",
    "                tf_idf_dic[tuple_pair[0]].append(tuple_pair[1])\n",
    "\n",
    "    print(result_set)\n",
    "    result_set=finalize_output(result_set,user_query_tfidf,tf_idf_dic,k)\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_tuples(user_query_tfidf,tf_idf_dic):\n",
    "    cosine_sim_lst_tuples=[]\n",
    "    for key,value in tf_idf_dic.items():\n",
    "        tf_idf_val=cosine_similarity([user_query_tfidf],[value])[0][0]\n",
    "        cosine_sim_lst_tuples.append((tf_idf_val,key))\n",
    "    return cosine_sim_lst_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heapify_tuples(cosine_sim_lst_tuples,k):\n",
    "    heap = []\n",
    "    for item in cosine_sim_lst_tuples:\n",
    "         heappush(heap, item)\n",
    "    return wanted_doc(nlargest(k,heap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wanted_doc(heap_k_docs):\n",
    "    wanted_doc_ids={}\n",
    "    for tup in (heap_k_docs):\n",
    "        wanted_doc_ids[tup[1]]=round(tup[0],2)\n",
    "    return wanted_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_output(result_set,user_query_tfidf,tf_idf_dic,k):\n",
    "    cosine_sim_lst_tuples=cosine_sim_tuples(user_query_tfidf,tf_idf_dic)\n",
    "    wanted_doc_ids=heapify_tuples(cosine_sim_lst_tuples,k)\n",
    "    result_set=wanted_doc_ids.keys()\n",
    "    df=pd.DataFrame()\n",
    "\n",
    "    for i,val in enumerate(result_set):\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "        df=df.append(pd.read_csv('data/doc_'+str(val)+'.tsv',sep='\\t',usecols=['description','title','city','url']\n",
    "                                 ,encoding='ISO-8859-1',index_col=False))\n",
    "        df.reset_index().drop('index',axis=1)\n",
    "    df['similarity']=wanted_doc_ids.values()\n",
    "    return df#[['title','description','city','url','similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a, b):\n",
    "    cosine_similarity=(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "    return 1 - cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room bedroom bathroom\n",
      "[0, 1, 3, 7]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Stylish, fully remodeled home in upscale NW  Alamo Heights Area. \\n\\nAmazing location - House conveniently located in quiet street, with beautiful seasoned trees, prestigious neighborhood and very close to the airport, 281, 410 loop and down-town area. \\n\\nFeaturing an open floor plan, original hardwood floors, 3 bedrooms, 3 FULL bathrooms + an independent garden-TV room which can sleep 2 more\\n\\nEuropean inspired kitchen and top of the line decor. Driveway can park 4 cars.</td>\n",
       "      <td>Unique Location! Alamo Heights - Designer Inspired</td>\n",
       "      <td>https://www.airbnb.com/rooms/17481455?location=Cibolo%2C%20TX</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>This is a beautiful bedroom with a queen size bed and closet. We do not have pets and the house is always clean. The bathroom is shared and supplies such as towels and shampoo are available. We are only some miles from Downtown, TCU, TCC, and Stockyards.</td>\n",
       "      <td>Friendly Private Room in ?Quiet Neighborhood</td>\n",
       "      <td>https://www.airbnb.com/rooms/18977363?location=Cleburne%2C%20TX</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bryan</td>\n",
       "      <td>Private bedroom in a cute little home situated in the coveted Garden Acres neighborhood in Bryan. The bedroom has its own private access and its own private bathroom.</td>\n",
       "      <td>Private Room Close to Campus</td>\n",
       "      <td>https://www.airbnb.com/rooms/11839729?location=College%20Station%2C%20TX</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Humble</td>\n",
       "      <td>Welcome to stay in private room with queen bed and detached private bathroom on the second floor. Another private bedroom with sofa bed is available for additional guests. 10$ for an additional guest.\\n10min from IAH airport\\nAirport pick-up/drop off is available for $10/trip.</td>\n",
       "      <td>2 Private rooms/bathroom 10min from IAH airport</td>\n",
       "      <td>https://www.airbnb.com/rooms/18520444?location=Cleveland%2C%20TX</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city  \\\n",
       "0  San Antonio   \n",
       "0  Fort Worth    \n",
       "0  Bryan         \n",
       "0  Humble        \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         description  \\\n",
       "0  Stylish, fully remodeled home in upscale NW  Alamo Heights Area. \\n\\nAmazing location - House conveniently located in quiet street, with beautiful seasoned trees, prestigious neighborhood and very close to the airport, 281, 410 loop and down-town area. \\n\\nFeaturing an open floor plan, original hardwood floors, 3 bedrooms, 3 FULL bathrooms + an independent garden-TV room which can sleep 2 more\\n\\nEuropean inspired kitchen and top of the line decor. Driveway can park 4 cars.   \n",
       "0  This is a beautiful bedroom with a queen size bed and closet. We do not have pets and the house is always clean. The bathroom is shared and supplies such as towels and shampoo are available. We are only some miles from Downtown, TCU, TCC, and Stockyards.                                                                                                                                                                                                                                      \n",
       "0  Private bedroom in a cute little home situated in the coveted Garden Acres neighborhood in Bryan. The bedroom has its own private access and its own private bathroom.                                                                                                                                                                                                                                                                                                                              \n",
       "0  Welcome to stay in private room with queen bed and detached private bathroom on the second floor. Another private bedroom with sofa bed is available for additional guests. 10$ for an additional guest.\\n10min from IAH airport\\nAirport pick-up/drop off is available for $10/trip.                                                                                                                                                                                                               \n",
       "\n",
       "                                                title  \\\n",
       "0  Unique Location! Alamo Heights - Designer Inspired   \n",
       "0  Friendly Private Room in ?Quiet Neighborhood         \n",
       "0  Private Room Close to Campus                         \n",
       "0  2 Private rooms/bathroom 10min from IAH airport      \n",
       "\n",
       "                                                                        url  \\\n",
       "0  https://www.airbnb.com/rooms/17481455?location=Cibolo%2C%20TX              \n",
       "0  https://www.airbnb.com/rooms/18977363?location=Cleburne%2C%20TX            \n",
       "0  https://www.airbnb.com/rooms/11839729?location=College%20Station%2C%20TX   \n",
       "0  https://www.airbnb.com/rooms/18520444?location=Cleveland%2C%20TX           \n",
       "\n",
       "   similarity  \n",
       "0  0.99        \n",
       "0  0.99        \n",
       "0  0.96        \n",
       "0  0.94        "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define a new score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vocabs = pickle.load(open(\"doc_vocabs.p\", \"rb\"))\n",
    "vocabulary = pickle.load(open(\"vocabulary.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx = pickle.load(open(\"inverted_idx.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['dalla']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['bedroom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 23,\n",
       " 25,\n",
       " 33,\n",
       " 44,\n",
       " 47,\n",
       " 48,\n",
       " 52,\n",
       " 55,\n",
       " 57,\n",
       " 58,\n",
       " 64,\n",
       " 66,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 80,\n",
       " 85,\n",
       " 91,\n",
       " 97,\n",
       " 98,\n",
       " 109,\n",
       " 114,\n",
       " 118,\n",
       " 119,\n",
       " 126,\n",
       " 129,\n",
       " 134,\n",
       " 138,\n",
       " 142,\n",
       " 145,\n",
       " 150,\n",
       " 151,\n",
       " 153,\n",
       " 159,\n",
       " 160,\n",
       " 163,\n",
       " 164,\n",
       " 168,\n",
       " 169,\n",
       " 172,\n",
       " 173,\n",
       " 175,\n",
       " 176,\n",
       " 178,\n",
       " 179,\n",
       " 182,\n",
       " 184,\n",
       " 187,\n",
       " 190,\n",
       " 194,\n",
       " 195,\n",
       " 200,\n",
       " 206,\n",
       " 209,\n",
       " 211,\n",
       " 213,\n",
       " 220,\n",
       " 223,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 235,\n",
       " 237,\n",
       " 239,\n",
       " 240,\n",
       " 249,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 258,\n",
       " 259,\n",
       " 264,\n",
       " 275,\n",
       " 277,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 284,\n",
       " 288,\n",
       " 294,\n",
       " 297,\n",
       " 301,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 317,\n",
       " 318,\n",
       " 325,\n",
       " 327,\n",
       " 329,\n",
       " 332,\n",
       " 335,\n",
       " 339,\n",
       " 342,\n",
       " 344,\n",
       " 347,\n",
       " 349,\n",
       " 363,\n",
       " 368,\n",
       " 373,\n",
       " 378,\n",
       " 382,\n",
       " 384,\n",
       " 386,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 397,\n",
       " 407,\n",
       " 409,\n",
       " 411,\n",
       " 416,\n",
       " 418,\n",
       " 423,\n",
       " 428,\n",
       " 430,\n",
       " 433,\n",
       " 438,\n",
       " 442,\n",
       " 445,\n",
       " 447,\n",
       " 448,\n",
       " 451,\n",
       " 454,\n",
       " 455,\n",
       " 461,\n",
       " 463,\n",
       " 468,\n",
       " 470,\n",
       " 472,\n",
       " 474,\n",
       " 477,\n",
       " 478,\n",
       " 480,\n",
       " 489,\n",
       " 493,\n",
       " 498,\n",
       " 501,\n",
       " 506,\n",
       " 509,\n",
       " 510,\n",
       " 512,\n",
       " 515,\n",
       " 516,\n",
       " 522,\n",
       " 524,\n",
       " 527,\n",
       " 528,\n",
       " 533,\n",
       " 534,\n",
       " 536,\n",
       " 541,\n",
       " 544,\n",
       " 545,\n",
       " 547,\n",
       " 553,\n",
       " 554,\n",
       " 557,\n",
       " 559,\n",
       " 561,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 572,\n",
       " 573,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 586,\n",
       " 587,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 597,\n",
       " 599,\n",
       " 604,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 614,\n",
       " 616,\n",
       " 620,\n",
       " 621,\n",
       " 627,\n",
       " 631,\n",
       " 632,\n",
       " 645,\n",
       " 648,\n",
       " 654,\n",
       " 656,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 665,\n",
       " 669,\n",
       " 674,\n",
       " 675,\n",
       " 678,\n",
       " 687,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 694,\n",
       " 700,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 711,\n",
       " 712,\n",
       " 717,\n",
       " 718,\n",
       " 721,\n",
       " 722,\n",
       " 725,\n",
       " 730,\n",
       " 731,\n",
       " 733,\n",
       " 734,\n",
       " 741,\n",
       " 742,\n",
       " 744,\n",
       " 746,\n",
       " 749,\n",
       " 762,\n",
       " 765,\n",
       " 766,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 779,\n",
       " 783,\n",
       " 786,\n",
       " 788,\n",
       " 792,\n",
       " 794,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 805,\n",
       " 807,\n",
       " 818,\n",
       " 823,\n",
       " 824,\n",
       " 827,\n",
       " 843,\n",
       " 845,\n",
       " 852,\n",
       " 855,\n",
       " 856,\n",
       " 866,\n",
       " 873,\n",
       " 879,\n",
       " 882,\n",
       " 883,\n",
       " 885,\n",
       " 887,\n",
       " 891,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 909,\n",
       " 912,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 918,\n",
       " 920,\n",
       " 926,\n",
       " 927,\n",
       " 933,\n",
       " 934,\n",
       " 937,\n",
       " 940,\n",
       " 944,\n",
       " 945,\n",
       " 948,\n",
       " 959,\n",
       " 961,\n",
       " 968,\n",
       " 973,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 982,\n",
       " 983,\n",
       " 989,\n",
       " 996,\n",
       " 997,\n",
       " 1005,\n",
       " 1007,\n",
       " 1009,\n",
       " 1012,\n",
       " 1016,\n",
       " 1017,\n",
       " 1021,\n",
       " 1022,\n",
       " 1024,\n",
       " 1029,\n",
       " 1031,\n",
       " 1036,\n",
       " 1040,\n",
       " 1043,\n",
       " 1044,\n",
       " 1046,\n",
       " 1048,\n",
       " 1053,\n",
       " 1057,\n",
       " 1058,\n",
       " 1059,\n",
       " 1061,\n",
       " 1064,\n",
       " 1065,\n",
       " 1070,\n",
       " 1075,\n",
       " 1077,\n",
       " 1078,\n",
       " 1080,\n",
       " 1084,\n",
       " 1085,\n",
       " 1089,\n",
       " 1097,\n",
       " 1103,\n",
       " 1119,\n",
       " 1122,\n",
       " 1126,\n",
       " 1129,\n",
       " 1131,\n",
       " 1132,\n",
       " 1135,\n",
       " 1146,\n",
       " 1149,\n",
       " 1152,\n",
       " 1157,\n",
       " 1158,\n",
       " 1162,\n",
       " 1166,\n",
       " 1170,\n",
       " 1173,\n",
       " 1175,\n",
       " 1176,\n",
       " 1182,\n",
       " 1183,\n",
       " 1184,\n",
       " 1185,\n",
       " 1186,\n",
       " 1187,\n",
       " 1198,\n",
       " 1205,\n",
       " 1209,\n",
       " 1216,\n",
       " 1219,\n",
       " 1223,\n",
       " 1236,\n",
       " 1237,\n",
       " 1239,\n",
       " 1241,\n",
       " 1247,\n",
       " 1249,\n",
       " 1259,\n",
       " 1260,\n",
       " 1261,\n",
       " 1273,\n",
       " 1277,\n",
       " 1278,\n",
       " 1279,\n",
       " 1281,\n",
       " 1282,\n",
       " 1289,\n",
       " 1290,\n",
       " 1291,\n",
       " 1294,\n",
       " 1296,\n",
       " 1301,\n",
       " 1303,\n",
       " 1307,\n",
       " 1313,\n",
       " 1315,\n",
       " 1316,\n",
       " 1317,\n",
       " 1318,\n",
       " 1322,\n",
       " 1323,\n",
       " 1330,\n",
       " 1332,\n",
       " 1336,\n",
       " 1344,\n",
       " 1346,\n",
       " 1350,\n",
       " 1351,\n",
       " 1353,\n",
       " 1359,\n",
       " 1369,\n",
       " 1373,\n",
       " 1376,\n",
       " 1377,\n",
       " 1380,\n",
       " 1382,\n",
       " 1384,\n",
       " 1385,\n",
       " 1388,\n",
       " 1390,\n",
       " 1391,\n",
       " 1395,\n",
       " 1398,\n",
       " 1400,\n",
       " 1405,\n",
       " 1409,\n",
       " 1417,\n",
       " 1419,\n",
       " 1420,\n",
       " 1421,\n",
       " 1423,\n",
       " 1424,\n",
       " 1433,\n",
       " 1439,\n",
       " 1442,\n",
       " 1444,\n",
       " 1449,\n",
       " 1453,\n",
       " 1468,\n",
       " 1476,\n",
       " 1478,\n",
       " 1481,\n",
       " 1483,\n",
       " 1484,\n",
       " 1497,\n",
       " 1499,\n",
       " 1500,\n",
       " 1501,\n",
       " 1502,\n",
       " 1504,\n",
       " 1505,\n",
       " 1508,\n",
       " 1513,\n",
       " 1516,\n",
       " 1521,\n",
       " 1528,\n",
       " 1529,\n",
       " 1530,\n",
       " 1538,\n",
       " 1543,\n",
       " 1550,\n",
       " 1552,\n",
       " 1554,\n",
       " 1555,\n",
       " 1561,\n",
       " 1565,\n",
       " 1570,\n",
       " 1572,\n",
       " 1576,\n",
       " 1578,\n",
       " 1580,\n",
       " 1581,\n",
       " 1582,\n",
       " 1583,\n",
       " 1586,\n",
       " 1587,\n",
       " 1589,\n",
       " 1590,\n",
       " 1591,\n",
       " 1593,\n",
       " 1595,\n",
       " 1597,\n",
       " 1598,\n",
       " 1602,\n",
       " 1605,\n",
       " 1606,\n",
       " 1607,\n",
       " 1611,\n",
       " 1612,\n",
       " 1617,\n",
       " 1620,\n",
       " 1621,\n",
       " 1624,\n",
       " 1626,\n",
       " 1645,\n",
       " 1649,\n",
       " 1651,\n",
       " 1652,\n",
       " 1654,\n",
       " 1657,\n",
       " 1663,\n",
       " 1664,\n",
       " 1666,\n",
       " 1667,\n",
       " 1668,\n",
       " 1673,\n",
       " 1674,\n",
       " 1675,\n",
       " 1677,\n",
       " 1681,\n",
       " 1683,\n",
       " 1686,\n",
       " 1689,\n",
       " 1690,\n",
       " 1698,\n",
       " 1699,\n",
       " 1703,\n",
       " 1706,\n",
       " 1707,\n",
       " 1708,\n",
       " 1711,\n",
       " 1713,\n",
       " 1714,\n",
       " 1717,\n",
       " 1724,\n",
       " 1732,\n",
       " 1734,\n",
       " 1737,\n",
       " 1738,\n",
       " 1742,\n",
       " 1743,\n",
       " 1747,\n",
       " 1753,\n",
       " 1755,\n",
       " 1758,\n",
       " 1765,\n",
       " 1766,\n",
       " 1769,\n",
       " 1770,\n",
       " 1772,\n",
       " 1773,\n",
       " 1775,\n",
       " 1778,\n",
       " 1780,\n",
       " 1781,\n",
       " 1788,\n",
       " 1790,\n",
       " 1794,\n",
       " 1797,\n",
       " 1798,\n",
       " 1799,\n",
       " 1801,\n",
       " 1802,\n",
       " 1803,\n",
       " 1804,\n",
       " 1811,\n",
       " 1813,\n",
       " 1818,\n",
       " 1819,\n",
       " 1821,\n",
       " 1823,\n",
       " 1826,\n",
       " 1830,\n",
       " 1836,\n",
       " 1839,\n",
       " 1841,\n",
       " 1844,\n",
       " 1850,\n",
       " 1852,\n",
       " 1853,\n",
       " 1857,\n",
       " 1859,\n",
       " 1860,\n",
       " 1862,\n",
       " 1863,\n",
       " 1870,\n",
       " 1877,\n",
       " 1880,\n",
       " 1888,\n",
       " 1891,\n",
       " 1895,\n",
       " 1896,\n",
       " 1898,\n",
       " 1899,\n",
       " 1900,\n",
       " 1901,\n",
       " 1902,\n",
       " 1910,\n",
       " 1911,\n",
       " 1916,\n",
       " 1917,\n",
       " 1919,\n",
       " 1923,\n",
       " 1928,\n",
       " 1937,\n",
       " 1945,\n",
       " 1949,\n",
       " 1950,\n",
       " 1956,\n",
       " 1959,\n",
       " 1960,\n",
       " 1962,\n",
       " 1966,\n",
       " 1970,\n",
       " 1972,\n",
       " 1975,\n",
       " 1976,\n",
       " 1978,\n",
       " 1981,\n",
       " 1991,\n",
       " 1996,\n",
       " 2001,\n",
       " 2003,\n",
       " 2007,\n",
       " 2008,\n",
       " 2010,\n",
       " 2012,\n",
       " 2015,\n",
       " 2016,\n",
       " 2022,\n",
       " 2034,\n",
       " 2040,\n",
       " 2041,\n",
       " 2053,\n",
       " 2057,\n",
       " 2061,\n",
       " 2066,\n",
       " 2067,\n",
       " 2069,\n",
       " 2070,\n",
       " 2071,\n",
       " 2075,\n",
       " 2079,\n",
       " 2084,\n",
       " 2086,\n",
       " 2095,\n",
       " 2097,\n",
       " 2098,\n",
       " 2100,\n",
       " 2103,\n",
       " 2104,\n",
       " 2105,\n",
       " 2118,\n",
       " 2119,\n",
       " 2122,\n",
       " 2123,\n",
       " 2125,\n",
       " 2128,\n",
       " 2131,\n",
       " 2133,\n",
       " 2135,\n",
       " 2136,\n",
       " 2137,\n",
       " 2139,\n",
       " 2140,\n",
       " 2141,\n",
       " 2143,\n",
       " 2144,\n",
       " 2146,\n",
       " 2148,\n",
       " 2152,\n",
       " 2161,\n",
       " 2165,\n",
       " 2167,\n",
       " 2175,\n",
       " 2181,\n",
       " 2187,\n",
       " 2189,\n",
       " 2190,\n",
       " 2194,\n",
       " 2199,\n",
       " 2202,\n",
       " 2203,\n",
       " 2213,\n",
       " 2217,\n",
       " 2219,\n",
       " 2220,\n",
       " 2226,\n",
       " 2229,\n",
       " 2230,\n",
       " 2232,\n",
       " 2235,\n",
       " 2237,\n",
       " 2238,\n",
       " 2244,\n",
       " 2245,\n",
       " 2246,\n",
       " 2251,\n",
       " 2261,\n",
       " 2262,\n",
       " 2263,\n",
       " 2267,\n",
       " 2268,\n",
       " 2273,\n",
       " 2274,\n",
       " 2277,\n",
       " 2279,\n",
       " 2280,\n",
       " 2283,\n",
       " 2285,\n",
       " 2287,\n",
       " 2288,\n",
       " 2289,\n",
       " 2292,\n",
       " 2296,\n",
       " 2297,\n",
       " 2302,\n",
       " 2313,\n",
       " 2314,\n",
       " 2315,\n",
       " 2328,\n",
       " 2329,\n",
       " 2337,\n",
       " 2342,\n",
       " 2343,\n",
       " 2350,\n",
       " 2351,\n",
       " 2356,\n",
       " 2358,\n",
       " 2359,\n",
       " 2363,\n",
       " 2365,\n",
       " 2372,\n",
       " 2375,\n",
       " 2379,\n",
       " 2381,\n",
       " 2385,\n",
       " 2386,\n",
       " 2387,\n",
       " 2388,\n",
       " 2392,\n",
       " 2394,\n",
       " 2398,\n",
       " 2403,\n",
       " 2405,\n",
       " 2408,\n",
       " 2410,\n",
       " 2413,\n",
       " 2417,\n",
       " 2419,\n",
       " 2423,\n",
       " 2424,\n",
       " 2426,\n",
       " 2428,\n",
       " 2430,\n",
       " 2432,\n",
       " 2433,\n",
       " 2435,\n",
       " 2439,\n",
       " 2440,\n",
       " 2441,\n",
       " 2444,\n",
       " 2446,\n",
       " 2447,\n",
       " 2456,\n",
       " 2460,\n",
       " 2466,\n",
       " 2467,\n",
       " 2469,\n",
       " 2472,\n",
       " 2476,\n",
       " 2477,\n",
       " 2478,\n",
       " 2479,\n",
       " 2481,\n",
       " 2482,\n",
       " 2486,\n",
       " 2487,\n",
       " 2489,\n",
       " 2490,\n",
       " 2492,\n",
       " 2493,\n",
       " 2495,\n",
       " 2500,\n",
       " 2507,\n",
       " 2514,\n",
       " 2515,\n",
       " 2520,\n",
       " 2523,\n",
       " 2524,\n",
       " 2536,\n",
       " 2537,\n",
       " 2539,\n",
       " 2542,\n",
       " 2544,\n",
       " 2546,\n",
       " 2547,\n",
       " 2550,\n",
       " 2553,\n",
       " 2554,\n",
       " 2555,\n",
       " 2562,\n",
       " 2565,\n",
       " 2574,\n",
       " 2577,\n",
       " 2582,\n",
       " 2593,\n",
       " 2594,\n",
       " 2595,\n",
       " 2597,\n",
       " 2599,\n",
       " 2600,\n",
       " 2601,\n",
       " 2604,\n",
       " 2605,\n",
       " 2606,\n",
       " 2607,\n",
       " 2608,\n",
       " 2610,\n",
       " 2614,\n",
       " 2619,\n",
       " 2631,\n",
       " 2632,\n",
       " 2638,\n",
       " 2640,\n",
       " 2641,\n",
       " 2650,\n",
       " 2653,\n",
       " 2654,\n",
       " 2657,\n",
       " 2660,\n",
       " 2671,\n",
       " 2673,\n",
       " 2674,\n",
       " 2678,\n",
       " 2683,\n",
       " 2685,\n",
       " 2690,\n",
       " 2705,\n",
       " 2706,\n",
       " 2707,\n",
       " 2708,\n",
       " 2710,\n",
       " 2714,\n",
       " 2716,\n",
       " 2718,\n",
       " 2730,\n",
       " 2732,\n",
       " 2734,\n",
       " 2739,\n",
       " 2742,\n",
       " 2743,\n",
       " 2748,\n",
       " 2751,\n",
       " 2754,\n",
       " 2757,\n",
       " 2760,\n",
       " 2761,\n",
       " 2765,\n",
       " 2769,\n",
       " 2770,\n",
       " 2775,\n",
       " 2783,\n",
       " 2797,\n",
       " 2803,\n",
       " 2804,\n",
       " 2813,\n",
       " 2818,\n",
       " 2821,\n",
       " 2823,\n",
       " 2825,\n",
       " 2828,\n",
       " 2832,\n",
       " 2839,\n",
       " 2840,\n",
       " 2844,\n",
       " 2846,\n",
       " 2851,\n",
       " 2852,\n",
       " 2854,\n",
       " 2855,\n",
       " 2861,\n",
       " 2862,\n",
       " 2863,\n",
       " 2869,\n",
       " 2870,\n",
       " 2871,\n",
       " 2873,\n",
       " 2875,\n",
       " 2876,\n",
       " 2882,\n",
       " 2883,\n",
       " 2884,\n",
       " 2885,\n",
       " 2887,\n",
       " 2888,\n",
       " 2892,\n",
       " 2896,\n",
       " 2897,\n",
       " 2899,\n",
       " 2901,\n",
       " 2902,\n",
       " 2907,\n",
       " 2911,\n",
       " 2912,\n",
       " 2917,\n",
       " 2923,\n",
       " 2924,\n",
       " 2925,\n",
       " 2927,\n",
       " 2930,\n",
       " 2932,\n",
       " 2933,\n",
       " 2940,\n",
       " 2946,\n",
       " 2948,\n",
       " 2953,\n",
       " 2954,\n",
       " 2960,\n",
       " 2965,\n",
       " 2972,\n",
       " 2973,\n",
       " 2982,\n",
       " 2983,\n",
       " 2985,\n",
       " 2987,\n",
       " 2991,\n",
       " 2994,\n",
       " 2998,\n",
       " 3000,\n",
       " 3001,\n",
       " 3003,\n",
       " 3006,\n",
       " 3010,\n",
       " 3012,\n",
       " 3015,\n",
       " 3018,\n",
       " 3020,\n",
       " 3025,\n",
       " 3031,\n",
       " 3032,\n",
       " 3037,\n",
       " 3039,\n",
       " 3040,\n",
       " 3045,\n",
       " 3049,\n",
       " 3053,\n",
       " 3054,\n",
       " 3057,\n",
       " 3059,\n",
       " 3063,\n",
       " 3066,\n",
       " 3075,\n",
       " 3079,\n",
       " 3084,\n",
       " 3089,\n",
       " 3090,\n",
       " 3093,\n",
       " 3094,\n",
       " 3095,\n",
       " 3102,\n",
       " 3104,\n",
       " 3106,\n",
       " 3110,\n",
       " 3111,\n",
       " 3113,\n",
       " 3115,\n",
       " 3116,\n",
       " 3118,\n",
       " 3119,\n",
       " 3121,\n",
       " 3122,\n",
       " 3123,\n",
       " 3124,\n",
       " 3125,\n",
       " 3126,\n",
       " 3127,\n",
       " 3128,\n",
       " 3140,\n",
       " 3143,\n",
       " 3144,\n",
       " 3147,\n",
       " 3151,\n",
       " 3152,\n",
       " 3153,\n",
       " 3154,\n",
       " 3157,\n",
       " 3163,\n",
       " 3164,\n",
       " 3166,\n",
       " 3167,\n",
       " 3177,\n",
       " 3180,\n",
       " 3184,\n",
       " 3185,\n",
       " 3186,\n",
       " 3187,\n",
       " 3188,\n",
       " 3190,\n",
       " 3193,\n",
       " 3195,\n",
       " 3198,\n",
       " 3199,\n",
       " 3202,\n",
       " 3203,\n",
       " 3204,\n",
       " 3206,\n",
       " 3207,\n",
       " 3215,\n",
       " 3217,\n",
       " 3220,\n",
       " 3221,\n",
       " 3227,\n",
       " 3228,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_idx[vocabulary['bedroom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
